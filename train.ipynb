{"cells":[{"cell_type":"markdown","metadata":{"id":"liEjaeFZsunt"},"source":["<h3> Notebook Overview </h3>\n","\n","- 1) Load pre-processed source and target tensors (train + test)\n","- 2) Load pre-trained tokenizers from json files\n","- 3) Load embedding matrices for source and target languages\n","- 4) Create Tensorflow dataset\n","- 5) Instantiate model's components -- Encoder, BahdanauAttention, Decoder (from subclasses in my own module \"model_components\")\n","- 6) Functions for computing loss and gradients\n","- 7) Set up checkpoints for training\n","- 8) Train\n","- 9) Save trained model"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":839,"status":"ok","timestamp":1664009605494,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"4lp4b_Ytsunw"},"outputs":[],"source":["import numpy as np\n","import json\n","from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n","import tensorflow as tf\n","import time\n","import seaborn as sns"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22990,"status":"ok","timestamp":1664009630560,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"NtR2PIwLsunx","outputId":"c5d8781e-2c67-4f98-8e4a-7789f35a5f2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Colab Notebooks/colab_upload\n"]}],"source":["# # For colab\n","# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","# %cd gdrive/MyDrive/Colab Notebooks/colab_upload"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1331,"status":"ok","timestamp":1664009634600,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"23Et83mZs3tT"},"outputs":[],"source":["# Import from my own module\n","from model_components import Encoder, BahdanauAttention, Decoder"]},{"cell_type":"markdown","metadata":{"id":"RNqz1CvOsunx"},"source":["<h3> 1) Load source and target tensors (train + test) </h3>"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":29516,"status":"ok","timestamp":1664009670596,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"3qr6XMAisuny"},"outputs":[],"source":["# ## For colab\n","\n","# file_path = '/content/gdrive/MyDrive/attention_data/tensors/'\n","\n","# source_train_tensor = np.loadtxt(file_path + 'source_train_tensor.csv', delimiter = ',', dtype = 'int32')\n","# source_test_tensor = np.loadtxt(file_path + 'source_test_tensor.csv', delimiter = ',', dtype = 'int32')\n","# target_train_tensor = np.loadtxt(file_path + 'target_train_tensor.csv', delimiter = ',', dtype = 'int32')\n","# target_test_tensor = np.loadtxt(file_path + 'target_test_tensor.csv', delimiter = ',', dtype = 'int32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXDrOhTHsuny"},"outputs":[],"source":["# load source & train arrays from csv file:\n","source_train_tensor = np.loadtxt('tensors/source_train_tensor.csv', delimiter = ',', dtype = 'int32')\n","source_test_tensor = np.loadtxt('tensors/source_test_tensor.csv', delimiter = ',', dtype = 'int32')\n","target_train_tensor = np.loadtxt('tensors/target_train_tensor.csv', delimiter = ',', dtype = 'int32')\n","target_test_tensor = np.loadtxt('tensors/target_test_tensor.csv', delimiter = ',', dtype = 'int32')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":825,"status":"ok","timestamp":1664009685200,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"LOXLBTYZsunz","outputId":"770e2a50-224a-4cf2-d8e6-bfe920eddbb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["77 103\n"]}],"source":["max_source_length= max(len(t) for t in np.concatenate((source_train_tensor, source_test_tensor), axis=0))\n","max_target_length= max(len(t) for t in np.concatenate((target_train_tensor, target_test_tensor), axis=0))\n","\n","print(max_source_length, max_target_length)"]},{"cell_type":"markdown","metadata":{"id":"5Wx6UF2Isun0"},"source":["<h3> 2) Load pre-trained tokenizers from json files </h3>"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2969,"status":"ok","timestamp":1664009694548,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"unolRy-csun0"},"outputs":[],"source":["with open ('tokenizers/source_sentence_tokenizer.json') as f:\n","    data = json.load(f)\n","    source_sentence_tokenizer = tokenizer_from_json(data)\n","\n","with open ('tokenizers/target_sentence_tokenizer.json') as f:\n","    data = json.load(f)\n","    target_sentence_tokenizer = tokenizer_from_json(data)"]},{"cell_type":"markdown","metadata":{"id":"PEc9Pezqsun1"},"source":["- Create word-to-index and index-to-word mappings for source and target languages"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":542,"status":"ok","timestamp":1664009697000,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"TX652cvtsun1"},"outputs":[],"source":["source_word_index = source_sentence_tokenizer.word_index\n","target_word_index = target_sentence_tokenizer.word_index\n","\n","source_index_word = source_sentence_tokenizer.index_word\n","target_index_word = target_sentence_tokenizer.index_word"]},{"cell_type":"markdown","metadata":{"id":"6FcK-L3usun2"},"source":["- Retrieve vocab size and number of tokens for source and target languages"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1664009699311,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"rPhnpYXCsun2"},"outputs":[],"source":["vocab_len_source = len(source_word_index.keys())\n","vocab_len_target = len(target_word_index.keys())\n","\n","num_tokens_source = vocab_len_source + 1\n","num_tokens_target = vocab_len_target + 1"]},{"cell_type":"markdown","metadata":{"id":"elvCkw5Jsun2"},"source":["<h3> 3) Load embedding matrices for source and target languages </h3>"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":8873,"status":"ok","timestamp":1664009717092,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"xawvYsFUsun2"},"outputs":[],"source":["# ## For colab\n","\n","# file_path = '/content/gdrive/MyDrive/attention_data/embeddings/'\n","\n","# embedding_matrix_source = np.loadtxt(file_path + 'embedding_matrix_source.csv', delimiter = ',', dtype = 'int32')\n","# embedding_matrix_target = np.loadtxt(file_path + 'embedding_matrix_target.csv', delimiter = ',', dtype = 'int32')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9QP_BN9sun3"},"outputs":[],"source":["# load embedding matrices\n","embedding_matrix_source = np.loadtxt('embeddings/embedding_matrix_source.csv', delimiter = ',', dtype = 'int32')\n","embedding_matrix_target = np.loadtxt('embeddings/embedding_matrix_target.csv', delimiter = ',', dtype = 'int32')\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":385,"status":"ok","timestamp":1664009720714,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"MD-LJQXqsun3","outputId":"e85e5b8b-36b3-4d49-eacc-ac147b874953"},"outputs":[{"name":"stdout","output_type":"stream","text":["96 300\n"]}],"source":["# Retrieve embedding dimensions for source and target languages\n","embedding_dim_source = embedding_matrix_source.shape[1]\n","embedding_dim_target = embedding_matrix_target.shape[1]\n","\n","print(embedding_dim_source, embedding_dim_target)"]},{"cell_type":"markdown","metadata":{"id":"_WxhPirhsun3"},"source":["<h3> 4) Create Tensorflow dataset </h3>"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3344,"status":"ok","timestamp":1664009726806,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"LRNOwpXGsun3","outputId":"92b26cd7-7dac-4604-9b5c-062117eb3d29"},"outputs":[{"name":"stdout","output_type":"stream","text":["(32, 77) (32, 103)\n","tf.Tensor(\n","[   1  230  221  193    5 1340   10   22  148    2    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0], shape=(77,), dtype=int32)\n"]}],"source":["BATCH_SIZE = 32\n","# Create Tensorflow dataset and shuffle\n","dataset = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BATCH_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","# Check dataset\n","source_batch, target_batch = next(iter(dataset))\n","print(source_batch.shape, target_batch.shape)\n","print(source_batch[1])"]},{"cell_type":"markdown","metadata":{"id":"_z4A12eHsun4"},"source":["<h3> 5) Instantiate NMT model's components </h3>"]},{"cell_type":"markdown","metadata":{"id":"Rd1cjVcVsun4"},"source":["- Define all arguments for the model components"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":367,"status":"ok","timestamp":1664009729793,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"602qSC_Rsun4"},"outputs":[],"source":["BATCH_SIZE = 32\n","BUFFER_SIZE = len(source_train_tensor)\n","steps_per_epoch= BUFFER_SIZE//BATCH_SIZE\n","\n","num_tokens_source = num_tokens_source\n","num_tokens_target = num_tokens_target\n","\n","embedding_dim_source = embedding_dim_source\n","embedding_dim_target = embedding_dim_target\n","\n","units = 256\n","attention_layer_units = 100"]},{"cell_type":"markdown","metadata":{"id":"IuLVtaYnsun4"},"source":["- Create Encoder, Attention, Decoder objects\n","- To see the code for the Encoder, BahdanauAttention, Decoder -- check \"model_components.py\""]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4101,"status":"ok","timestamp":1664009737057,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"XtD_7swSsun4","outputId":"5d0f0b15-f127-4e90-b4c9-1c7e3983f583"},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder sequential: (32, 77, 512)\n","Encoder final state_h: (32, 512)\n","Encoder final state_c: (32, 512)\n","Context vector: (batch size, units) (32, 512)\n","Attention weights: (batch_size, sequence_length, 1) (32, 77, 1)\n","Decoder output shape: (batch_size, vocab size) (32, 17410)\n"]}],"source":["encoder = Encoder(num_tokens_source, embedding_dim_source, units, BATCH_SIZE, embedding_matrix_source)\n","attention_layer= BahdanauAttention(attention_layer_units)\n","decoder = Decoder(num_tokens_target, embedding_dim_target, 2*units, BATCH_SIZE, embedding_matrix_target, attention_layer_units)\n","\n","# Note: We are passing in \"2 * units\" as the \"decoder_units\" argument for the Decoder, since the encoder uses bi-directional LSTMs,\n","# and we are feeding the final hidden and cell states of the Encoder as the initial hidden and cell states of the Decoder.\n","\n","\n","# Check dimensions are correct for Encoder, BahdanauAttention layer, and Decoder.\n","enc_sequential, enc_final_h, enc_final_c = encoder(source_batch)\n","print (f'Encoder sequential: {enc_sequential.shape}')\n","print (f'Encoder final state_h: {enc_final_h.shape}')\n","print (f'Encoder final state_c: {enc_final_c.shape}')\n","\n","attention_result, attention_weights = attention_layer(enc_final_h, enc_sequential)\n","print(f\"Context vector: (batch size, units) {attention_result.shape}\")\n","print(f\"Attention weights: (batch_size, sequence_length, 1) {attention_weights.shape}\")\n","\n","sample_decoder_output, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final_h, enc_final_c, enc_sequential)\n","print (f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')"]},{"cell_type":"markdown","metadata":{"id":"2-NGH282sun4"},"source":["- Create optimizer"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":842,"status":"ok","timestamp":1664009740400,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"2NiBetbnsun4"},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam()"]},{"cell_type":"markdown","metadata":{"id":"v4jc-ip-sun4"},"source":["<h3> 6) Loss and Gradients </h3>"]},{"cell_type":"markdown","metadata":{"id":"Q-HO7dH2sun5"},"source":["- define loss_function() (for computing loss per batch) and get_loss_and_grads() (for computing gradient of loss w.r.t variables)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":821,"status":"ok","timestamp":1664009743618,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"wE2kUz2-iTGX"},"outputs":[],"source":["def loss_function(real, pred):      \n","    \"\"\"\n","    Compute mean loss for batch\n","\n","    Arguments\n","    real -- (m, 1)\n","    pred -- (m, vocab_size)\n","\n","    Returns\n","    mean loss for batch\n","    \"\"\"\n","    \n","    # create mask, such that mask value = 1 when \"real\" value is non-zero\n","    mask = 1 - tf.cast(tf.experimental.numpy.equal(real, 0), tf.float32)\n","\n","    # compute cross-categorical entropy for values of \"real\" that are non-zero\n","    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = real, logits = pred) * mask\n","    return tf.reduce_mean(loss_)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1664009744814,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"0qJFddCvhgy8"},"outputs":[],"source":["# use tf.function decorator to activate graph mode instead of eager execution\n","# this improves performance considerably\n","@tf.function\n","def get_loss_and_grads(inp, targ):\n","    \"\"\"\n","    Compute gradient of loss (for batch) with respect to variables\n","    Loop over Ty and use teacher forcing (i.e. compare y_pred at time t with y_true at time t+1)\n","\n","    Arguments\n","    inp = (m, Tx)\n","    targ = (m, Ty)\n","\n","    Returns \n","    loss -- scalar\n","    gradients\n","    \"\"\"\n","\n","    loss = 0\n","    with tf.GradientTape() as tape:\n","        # retrieve Encoder outputs\n","        enc_sequential, enc_final_h, enc_final_c = encoder(inp)\n","\n","        # Initialise the Decoder (hidden + cell) states at time-step 0 with the final states of Encoder\n","        dec_h = enc_final_h\n","        dec_c = enc_final_c\n","\n","        # Initialise Decoder inputs at time-step 0 with \"start_\" token\n","        dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)    # (m, 1)\n","\n","        # Loop over Ty, starting from time-step = 1, to predict output iteratively\n","        for t in range(1, targ.shape[1]):\n","            # retrieve \"predictions\", \"dec_h\", \"dec_c\" for current time-step\n","            predictions, dec_h, dec_c, _ = decoder(dec_input, dec_h, dec_c, enc_sequential)     # predictions = (m, vocab_size)\n","            \n","            # Teacher forcing: compute loss by comparing \"predictions\" with ground-truth values at time-step t\n","            loss += loss_function(targ[:, t], predictions)\n","            \n","            # Teacher forcing: update \"dec_input\" with ground-truth values at time-step t\n","            dec_input = tf.expand_dims(targ[:, t], 1)\n","        \n","        # Now you have the loss for the current batch (\"inputs\")\n","        # Compute gradients (d_loss/d_variables)\n","        variables = encoder.variables + decoder.variables\n","        gradients = tape.gradient(loss, variables)\n","        return loss, gradients\n"]},{"cell_type":"markdown","metadata":{"id":"7Fxf9fsusun5"},"source":["<h3> 7) Set up checkpoints for training </h3>"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1664009748112,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"DAZp9aH6sun5","outputId":"c97dfb00-0ee2-463f-c567-ddadf259b1c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialising from scratch\n"]}],"source":["checkpoint_path = './checkpoints'\n","\n","ckpt = tf.train.Checkpoint(optimizer = optimizer,\n","                                 encoder = encoder, decoder = decoder)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 3)\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print('Latest checkpoint restored!')\n","else:\n","    print('Initialising from scratch')"]},{"cell_type":"markdown","metadata":{"id":"Z3958suhsun5"},"source":["<h3> 8) Train </h3>"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19345957,"status":"ok","timestamp":1664029127729,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"y3lmAYQGsun5","outputId":"5c8eae09-9f01-4667-ed18-a22db4a0eafe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 Batch 0 Loss 70.1638\n","Epoch 1 Batch 2000 Loss 20.7534\n","Epoch 1 Batch 4000 Loss 19.3555\n","Epoch 1 Batch 6000 Loss 18.7942\n","Epoch 1 Loss 21.8595\n","Time taken for 1 epoch 2006.8411746025085 sec\n","\n","Saving checkpoint after epoch 1 at ./checkpoints/ckpt-1\n","Epoch 2 Batch 0 Loss 11.7460\n","Epoch 2 Batch 2000 Loss 8.5506\n","Epoch 2 Batch 4000 Loss 10.1264\n","Epoch 2 Batch 6000 Loss 11.7742\n","Epoch 2 Loss 11.8022\n","Time taken for 1 epoch 1920.6357457637787 sec\n","\n","Saving checkpoint after epoch 2 at ./checkpoints/ckpt-2\n","Epoch 3 Batch 0 Loss 7.5941\n","Epoch 3 Batch 2000 Loss 6.3093\n","Epoch 3 Batch 4000 Loss 8.3682\n","Epoch 3 Batch 6000 Loss 9.4711\n","Epoch 3 Loss 8.4040\n","Time taken for 1 epoch 1922.575403213501 sec\n","\n","Saving checkpoint after epoch 3 at ./checkpoints/ckpt-3\n","Epoch 4 Batch 0 Loss 5.2665\n","Epoch 4 Batch 2000 Loss 5.8861\n","Epoch 4 Batch 4000 Loss 5.1178\n","Epoch 4 Batch 6000 Loss 6.9959\n","Epoch 4 Loss 6.5665\n","Time taken for 1 epoch 1921.3809475898743 sec\n","\n","Saving checkpoint after epoch 4 at ./checkpoints/ckpt-4\n","Epoch 5 Batch 0 Loss 3.7449\n","Epoch 5 Batch 2000 Loss 4.2967\n","Epoch 5 Batch 4000 Loss 4.4245\n","Epoch 5 Batch 6000 Loss 6.1171\n","Epoch 5 Loss 5.4185\n","Time taken for 1 epoch 1933.65159201622 sec\n","\n","Saving checkpoint after epoch 5 at ./checkpoints/ckpt-5\n","Epoch 6 Batch 0 Loss 3.7814\n","Epoch 6 Batch 2000 Loss 4.3234\n","Epoch 6 Batch 4000 Loss 4.3553\n","Epoch 6 Batch 6000 Loss 5.9147\n","Epoch 6 Loss 4.6050\n","Time taken for 1 epoch 1920.1531655788422 sec\n","\n","Saving checkpoint after epoch 6 at ./checkpoints/ckpt-6\n","Epoch 7 Batch 0 Loss 2.3008\n","Epoch 7 Batch 2000 Loss 3.4743\n","Epoch 7 Batch 4000 Loss 4.1336\n","Epoch 7 Batch 6000 Loss 6.7291\n","Epoch 7 Loss 3.9986\n","Time taken for 1 epoch 1933.640897989273 sec\n","\n","Saving checkpoint after epoch 7 at ./checkpoints/ckpt-7\n","Epoch 8 Batch 0 Loss 2.8089\n","Epoch 8 Batch 2000 Loss 2.9803\n","Epoch 8 Batch 4000 Loss 2.8467\n","Epoch 8 Batch 6000 Loss 5.0747\n","Epoch 8 Loss 3.5185\n","Time taken for 1 epoch 1927.969394683838 sec\n","\n","Saving checkpoint after epoch 8 at ./checkpoints/ckpt-8\n","Epoch 9 Batch 0 Loss 2.3021\n","Epoch 9 Batch 2000 Loss 2.1897\n","Epoch 9 Batch 4000 Loss 2.7298\n","Epoch 9 Batch 6000 Loss 3.3433\n","Epoch 9 Loss 3.1315\n","Time taken for 1 epoch 1929.9761517047882 sec\n","\n","Saving checkpoint after epoch 9 at ./checkpoints/ckpt-9\n","Epoch 10 Batch 0 Loss 2.9249\n","Epoch 10 Batch 2000 Loss 1.5500\n","Epoch 10 Batch 4000 Loss 2.7966\n","Epoch 10 Batch 6000 Loss 3.2169\n","Epoch 10 Loss 2.8213\n","Time taken for 1 epoch 1918.9732310771942 sec\n","\n","Saving checkpoint after epoch 10 at ./checkpoints/ckpt-10\n"]}],"source":["EPOCHS = 10\n","\n","# for every epoch\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","    total_loss = 0\n","    \n","    # for every batch\n","    for (batch, (inp, targ)) in enumerate(dataset):\n","        # compute loss and gradients for batch\n","        loss, gradients = get_loss_and_grads(inp, targ)\n","        # update total_loss\n","        total_loss += loss\n","        \n","        # use optimizer and gradients to update variables\n","        variables = encoder.variables + decoder.variables\n","        optimizer.apply_gradients(zip(gradients, variables))\n","\n","        if batch % 2000 == 0:\n","            print(f'Epoch {epoch + 1} Batch {batch} Loss {loss.numpy():.4f}')\n","    \n","    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n","    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')\n","    if (epoch+1) % 1 == 0:\n","        ckpt_save_path = ckpt_manager.save()\n","        print(f'Saving checkpoint after epoch {epoch + 1} at {ckpt_save_path}')\n","        "]},{"cell_type":"markdown","metadata":{"id":"vDKTMU1zsun6"},"source":["<h3> 9) Save model </h3>"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":818,"status":"ok","timestamp":1664029174627,"user":{"displayName":"kh oh","userId":"00014367513729162087"},"user_tz":-120},"id":"Sg1CgF_2sun6"},"outputs":[],"source":["file_path = 'saved_models/model'\n","encoder.save_weights(file_path + '/encoder',save_format='tf')\n","decoder.save_weights(file_path + '/decoder',save_format='tf')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"utqkfT6g3iS6"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('tf')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a30d934768a106a0bdaa0b54b1b0ce58ac936216b32c4f047caaf50b54e34c32"}}},"nbformat":4,"nbformat_minor":0}
