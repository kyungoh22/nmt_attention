{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- functioning NMT model\n",
    "- attention model using classes\n",
    "- bi-directional LSTMs in Encoder\n",
    "- training seems to work; loss generally going down over 3 epochs\n",
    "- make sure to use batch size = 32\n",
    "- Check the final evaluation function – needs to be adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, Concatenate, LSTM, Embedding, Dense\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "#import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])\n",
    "df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"'\", '', sentence)\n",
    "    sentence = sentence.replace('ü', 'ue').replace('ä', 'ae').replace('ö', 'oe').replace('ß', 'ss')\n",
    "    exclude = set(string.punctuation)\n",
    "    sentence = ''.join(ch for ch in sentence if ch not in exclude)\n",
    "    sentence = 'start_ ' + sentence + ' _end'\n",
    "    sentence = sentence.encode(\"ascii\", \"ignore\")\n",
    "    sentence = sentence.decode()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = df_en_de\n",
    "pairs = pairs.sample(frac = 0.01)\n",
    "pairs['english'] = pairs['english'].apply(preprocess_sentence)\n",
    "pairs['german'] = pairs['german'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = pairs['english']\n",
    "target = pairs ['german']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer & tensor for source and target\n",
    "source_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "source_sentence_tokenizer.fit_on_texts(source)\n",
    "source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n",
    "source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor, padding='post' )\n",
    "\n",
    "target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "target_sentence_tokenizer.fit_on_texts(target)\n",
    "target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n",
    "target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor = train_test_split(\n",
    "                                                                source_tensor, target_tensor,test_size=0.2\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_length= max(len(t) for t in  target_tensor)\n",
    "max_source_length= max(len(t) for t in source_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 23)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_length, max_source_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 08:35:43.152977: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "#Create data in memeory \n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BATCH_SIZE)\n",
    "# shuffles the data in the batch\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 23) (32, 24)\n",
      "tf.Tensor(\n",
      "[  1  50  36 218 156   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0], shape=(23,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "source_batch, target_batch =next(iter(dataset))\n",
    "print(source_batch.shape, target_batch.shape)\n",
    "print(source_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(source_train_tensor)\n",
    "steps_per_epoch= BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim=256\n",
    "units=128\n",
    "attention_layer_units = 100\n",
    "source_vocab_size= len(source_sentence_tokenizer.word_index)+1\n",
    "target_vocab_size= len(target_sentence_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm= Bidirectional(LSTM (encoder_units, \n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,                                      \n",
    "                                      recurrent_initializer='glorot_uniform'\n",
    "                                     ))\n",
    "    def call(self, x, ini_state):\n",
    "                                                                # x = (m, Tx)\n",
    "                                                                # hidden = (m, encoder_units)\n",
    "                                                                \n",
    "        x = self.embedding(x)                                   # x = (m, Tx, embedding_dim)\n",
    "        # pass input x through bi-directional LSTM\n",
    "                                                                \n",
    "        (enc_sequential, enc_forward_h, \n",
    "        enc_forward_c, enc_backward_h, enc_backward_c) = self.lstm(x, initial_state = [ini_state, ini_state, ini_state, ini_state])\n",
    "\n",
    "        # concatenate forward and backward states\n",
    "        enc_final_h = Concatenate()([enc_forward_h, enc_backward_h])\n",
    "        enc_final_c = Concatenate()([enc_forward_c, enc_backward_c])\n",
    "\n",
    "        return enc_sequential, enc_final_h, enc_final_c                     # enc_sequential = (m, Tx, 2 * encoder_units) \n",
    "                                                                            # enc_h = (m, 2 * encoder_units)\n",
    "                                                                            # enc_c = (m, 2 * encoder_units)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))\n",
    "    \n",
    "    # def initialize_cell_state(self):\n",
    "    #     return tf.zeros((self.batch_size, 2 * self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder sequential: (32, 23, 256)\n",
      "Encoder final state_h: (32, 256)\n",
      "Encoder final state_c: (32, 256)\n"
     ]
    }
   ],
   "source": [
    "# check dimensions\n",
    "encoder = Encoder(source_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "initial_state = encoder.initialize_state()\n",
    "enc_sequential, enc_final_h, enc_final_c = encoder(source_batch, initial_state)\n",
    "\n",
    "print (f'Encoder sequential: {enc_sequential.shape}')\n",
    "print (f'Encoder final state_h: {enc_final_h.shape}')\n",
    "print (f'Encoder final state_c: {enc_final_c.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super( BahdanauAttention, self).__init__()\n",
    "        self.W1= tf.keras.layers.Dense(units)  # decoder hidden (at time-step \"t-1\")\n",
    "        self.W2= tf.keras.layers.Dense(units)  # encoder hidden (at time-step \"t\")\n",
    "        self.V= tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, dec_hidden, enc_hidden):\n",
    "                                                                # dec_hidden = (m, 2*units) \n",
    "                                                                # enc_hidden:   (m, Tx, 2*units)\n",
    "\n",
    "        dec_hidden_with_time = tf.expand_dims(dec_hidden, 1)    # dec_hidden_with_time = (m, 1, 2*units)\n",
    "        \n",
    "                                                                \n",
    "        # W1() = (m, 1, 10) \n",
    "        # W2() = (m, Tx, 10)\n",
    "        # Broadcasting happens when you add\n",
    "        # W1() + W2 () = (m, Tx, 10)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(dec_hidden_with_time) + self.W2(enc_hidden))) # (m, Tx, 1)\n",
    "        \n",
    "        # normalise scores with softmax\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)                                # (m, Tx, 1)\n",
    "        \n",
    "        # apply each weight to encoder hidden state at respective time-step \n",
    "        context_vector= attention_weights * enc_hidden                                  # (m, Tx, 2*units)\n",
    "       \n",
    "        # linear combination of enc_hidden vectors for all Tx\n",
    "        # so sum along Tx axis\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)                          # (m, 2*units)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector: (batch size, units) (32, 256)\n",
      "attention weights: (batch_size, sequence_length, 1) (32, 23, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer= BahdanauAttention(attention_layer_units)\n",
    "attention_result, attention_weights = attention_layer(enc_final_h, enc_sequential)\n",
    "print(f\"context vector: (batch size, units) {attention_result.shape}\")\n",
    "print(f\"attention weights: (batch_size, sequence_length, 1) {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder for one time-step\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_sz):\n",
    "        super (Decoder,self).__init__()\n",
    "        self.batch_sz= batch_sz\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm= LSTM (decoder_units, \n",
    "                        return_sequences= True,\n",
    "                        return_state=True,\n",
    "                        recurrent_initializer='glorot_uniform')\n",
    "        # Fully connected layer\n",
    "        self.fc= Dense(vocab_size)      # Note, we don't use an activation here.\n",
    "                                        # For the calculation of the loss, we will use \n",
    "                                        # sparse_softmax_cross_entropy_with_logits, which performs \n",
    "                                        # the softmax on the logits internally for greater efficiency\n",
    "        \n",
    "        # attention\n",
    "        self.attention = BahdanauAttention(attention_layer_units)\n",
    "    \n",
    "    def call(self, y, dec_h, dec_c, enc_sequential):\n",
    "                                                                                    # dec_h: (m, 2*units) \n",
    "                                                                                    # dec_c: (m, 2*units)\n",
    "                                                                                    # enc_sequential: (m, Tx, 2*units) \n",
    "\n",
    "        context_vector, attention_weights = self.attention(dec_h, enc_sequential)   # context_vector = (m, 2*units)\n",
    "        \n",
    "        y= self.embedding(y)                                                        # y = (m, 1, embedding_dim)\n",
    "        \n",
    "        # concatenate context vector and embedding for output sequence\n",
    "        y = tf.concat([tf.expand_dims(context_vector, 1), y],                       # (m, 1, 2*units) + (m, 1, embedding_dim)\n",
    "                                      axis=-1)                                      # (m, 1, 2*units + embedding_dim)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, dec_h, dec_c = self.lstm(y, initial_state = [dec_h, dec_c])                                         # output = (m, 1, 2*units)\n",
    "                                                                                    # dec_h = (m, 2*units)\n",
    "                                                                                    # dec_c = (m, 2*units)\n",
    "\n",
    "        output= tf.reshape(output, (-1, output.shape[2]))                           # output = (m, 2*units)\n",
    "        \n",
    "        # pass the output thru Fc layers\n",
    "        y = self.fc(output)                                                         # y = (m, vocab_size)\n",
    "        return y, dec_h, dec_c, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (32, 3206)\n"
     ]
    }
   ],
   "source": [
    "# Make sure to pass in \"2*units\", since the encoder uses bi-directional LSTM\n",
    "# We're feeding final_h and final_c from Encoder as init_h and init_c for Decoder\n",
    "decoder= Decoder(target_vocab_size, embedding_dim, 2*units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final_h, enc_final_c, enc_sequential)\n",
    "print (f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer and the loss function\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):      # \"real\" = (m, 1), \"pred\" = (m, vocab_size)\n",
    "    mask = 1 - np.equal(real, 0)    # mask = 1 when \"real\" != 0\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.2702\n",
      "Epoch 1 Batch 50 Loss 1.7710\n",
      "Epoch 1 Loss 1.9602\n",
      "Time taken for 1 epoch 41.70515418052673 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "# initial_state = encoder.initialize_state()\n",
    "# enc_sequential, enc_final_h, enc_final_c = encoder(source_batch, initial_state)\n",
    "# sample_decoder_output, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final_h, enc_final_c, enc_sequential)\n",
    "# decoder returns: y, dec_h, dec_c, attention_weights\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    initial_state = encoder.initialize_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "                                                        # inp: (batch_size, Tx)\n",
    "                                                        # targ: (batch_size, Ty)\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_sequential, enc_final_h, enc_final_c = encoder(inp, initial_state)\n",
    "            \n",
    "            dec_h = enc_final_h\n",
    "            dec_c = enc_final_c\n",
    "            \n",
    "            dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)    # (m, 1)   \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_h, dec_c, _ = decoder(dec_input, dec_h, dec_c, enc_sequential) # predictions = (m, vocab_size)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # update dec_input for teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "    \n",
    "    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputs, encoder, decoder, max_source_length, max_target_length):\n",
    "    # inputs = (1, Tx)\n",
    "\n",
    "    \n",
    "    input_sentence = ''\n",
    "    for i in inputs[0]:\n",
    "        if i == 0:\n",
    "            break\n",
    "        input_sentence = input_sentence + source_sentence_tokenizer.index_word[i] + ' '\n",
    "    #input_sentence = input_sentence[:-1]\n",
    "    \n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    # Encoder: \n",
    "    # Input: x, init_state\n",
    "    # Return: enc_sequential, enc_final_h, enc_final_c\n",
    "\n",
    "    init_state = [tf.zeros((1, units))]\n",
    "    enc_sequential, enc_final_h, enc_final_c = encoder(inputs, init_state)\n",
    "\n",
    "    dec_h = enc_final_h\n",
    "    dec_c = enc_final_c\n",
    "    \n",
    "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)             # dec_input = (1, 1)\n",
    "\n",
    "    # Decoder:\n",
    "    # Input: y, dec_h, dec_c, enc_sequential\n",
    "    # Return: y, dec_h, dec_c, attention_weights\n",
    "\n",
    "    # start decoding\n",
    "    for t in range(max_target_length): # limit the length of the decoded sequence\n",
    "        predictions, dec_h, dec_c, attention_weights = decoder(dec_input, dec_h, dec_c, enc_sequential)    # predictions = (1, vocab_size)\n",
    "                                                                                                           # dec_h = (1, 2*units)\n",
    "                                                                                                           # dec_c = (1, 2*units)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        # stop decoding if '_end' is predicted\n",
    "        if target_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
    "            return result, input_sentence\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)                                         # dec_input = (1,1)  \n",
    "\n",
    "    return result, input_sentence\n",
    "  \n",
    "def predict_random_val_sentence():\n",
    "    \n",
    "    k = np.random.randint(len(source_train_tensor))\n",
    "    random_input = source_train_tensor[k]\n",
    "    random_output = target_train_tensor[k]\n",
    "    random_input = np.expand_dims(random_input,0)           # random_input = (1, Tx)\n",
    "    result, sentence = evaluate(random_input, encoder, decoder, max_source_length, max_target_length)\n",
    "    print(f'Input: {sentence[7:-5]}')                   # Want to skip \"start_ \" and \" _end\"\n",
    "    print(f'Predicted translation: {result[:-5]}')\n",
    "    true_translation = ''\n",
    "    for i in random_output:\n",
    "        if i == 0:\n",
    "            break\n",
    "        true_translation = true_translation + target_sentence_tokenizer.index_word[i] + ' '\n",
    "    true_translation = true_translation[7:-6]               # Want to skip \"start_\" and \" _end \"\n",
    "    print(f'Actual translation: {true_translation}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer \"forward_lstm\" (type LSTM).\n\n'list' object has no attribute 'shape'\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1, 23, 256), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=[['tf.Tensor(shape=(1, 128), dtype=float32)'], ['tf.Tensor(shape=(1, 128), dtype=float32)']]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m (predict_random_val_sentence())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m (predict_random_val_sentence())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m (predict_random_val_sentence())\n",
      "\u001b[1;32m/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb Cell 25\u001b[0m in \u001b[0;36mpredict_random_val_sentence\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=53'>54</a>\u001b[0m random_output \u001b[39m=\u001b[39m target_train_tensor[k]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=54'>55</a>\u001b[0m random_input \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(random_input,\u001b[39m0\u001b[39m)           \u001b[39m# random_input = (1, Tx)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=55'>56</a>\u001b[0m result, sentence \u001b[39m=\u001b[39m evaluate(random_input, encoder, decoder, max_source_length, max_target_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m{\u001b[39;00msentence[\u001b[39m7\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)                   \u001b[39m# Want to skip \"start_ \" and \" _end\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPredicted translation: \u001b[39m\u001b[39m{\u001b[39;00mresult[:\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb Cell 25\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(inputs, encoder, decoder, max_source_length, max_target_length)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=16'>17</a>\u001b[0m \u001b[39m# Encoder: \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=17'>18</a>\u001b[0m \u001b[39m# Input: x, init_state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=18'>19</a>\u001b[0m \u001b[39m# Return: enc_sequential, enc_final_h, enc_final_c\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=20'>21</a>\u001b[0m init_state \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, units))]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=21'>22</a>\u001b[0m enc_sequential, enc_final_h, enc_final_c \u001b[39m=\u001b[39m encoder(inputs, init_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=23'>24</a>\u001b[0m dec_h \u001b[39m=\u001b[39m enc_final_h\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=24'>25</a>\u001b[0m dec_c \u001b[39m=\u001b[39m enc_final_c\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;32m/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb Cell 25\u001b[0m in \u001b[0;36mEncoder.call\u001b[0;34m(self, x, ini_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)                                   \u001b[39m# x = (m, Tx, embedding_dim)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=16'>17</a>\u001b[0m \u001b[39m# pass input x through bi-directional LSTM\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=18'>19</a>\u001b[0m (enc_sequential, enc_forward_h, \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=19'>20</a>\u001b[0m enc_forward_c, enc_backward_h, enc_backward_c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, initial_state \u001b[39m=\u001b[39;49m [ini_state, ini_state, ini_state, ini_state])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=21'>22</a>\u001b[0m \u001b[39m# concatenate forward and backward states\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_2.ipynb#ch0000025?line=22'>23</a>\u001b[0m enc_final_h \u001b[39m=\u001b[39m Concatenate()([enc_forward_h, enc_backward_h])\n",
      "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer \"forward_lstm\" (type LSTM).\n\n'list' object has no attribute 'shape'\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1, 23, 256), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=[['tf.Tensor(shape=(1, 128), dtype=float32)'], ['tf.Tensor(shape=(1, 128), dtype=float32)']]"
     ]
    }
   ],
   "source": [
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ now just relax _end\n",
      "Predicted translation: ich habe eine gute idee _end \n",
      "Actual translation: start_ jetzt entspann dich einfach _end \n",
      "None\n",
      "Input: start_ she asked him to give her some money so she could go to a restaurant with her friends _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ sie bat ihn um geld damit sie mit ihren freunden in ein restaurant gehen koennte _end \n",
      "None\n",
      "Input: start_ lets see what we remember from the last lesson _end\n",
      "Predicted translation: tom hat eine gute idee _end \n",
      "Actual translation: start_ wir wollen mal sehen was von der letzten unterrichtsstunde noch haengen geblieben ist _end \n",
      "None\n",
      "Input: start_ is this house yours _end\n",
      "Predicted translation: das ist _end \n",
      "Actual translation: start_ ist das dein haus _end \n",
      "None\n",
      "Input: start_ this car belongs to tom _end\n",
      "Predicted translation: tom ist nicht sehr _end \n",
      "Actual translation: start_ dieser wagen gehoert tom _end \n",
      "None\n",
      "Input: start_ i will give you a bike for your birthday _end\n",
      "Predicted translation: ich habe eine gute idee _end \n",
      "Actual translation: start_ ich werde dir ein fahrrad zum geburtstag schenken _end \n",
      "None\n",
      "Input: start_ she won an oscar nomination for best supporting actress _end\n",
      "Predicted translation: der tuer _end \n",
      "Actual translation: start_ sie wurde fuer einen oscar als beste nebendarstellerin nominiert _end \n",
      "None\n",
      "Input: start_ many of the students got bored and fell asleep _end\n",
      "Predicted translation: tom hat sich der nacht hellwach _end \n",
      "Actual translation: start_ viele schueler schliefen vor langeweile ein _end \n",
      "None\n",
      "Input: start_ i cant believe that youre really in love with me _end\n",
      "Predicted translation: ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich \n",
      "Actual translation: start_ ich kann nicht glauben dass du wirklich in mich verliebt bist _end \n",
      "None\n",
      "Input: start_ tom couldnt see mary _end\n",
      "Predicted translation: tom _end \n",
      "Actual translation: start_ tom konnte maria nicht sprechen _end \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ i thought you were older than me _end\n",
      "Predicted translation: ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich \n",
      "Actual translation: start_ ich dachte du waerest aelter als ich _end \n",
      "None\n",
      "Input: start_ these are real _end\n",
      "Predicted translation: das ist ein guter gitarrist _end \n",
      "Actual translation: start_ die sind echt _end \n",
      "None\n",
      "Input: start_ do you think i dont care _end\n",
      "Predicted translation: ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich \n",
      "Actual translation: start_ glaubst du etwa das ist mir gleich _end \n",
      "None\n",
      "Input: start_ i dont understand why people idolize criminals _end\n",
      "Predicted translation: ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich \n",
      "Actual translation: start_ ich verstehe nicht warum manche leute verbrecher verehren _end \n",
      "None\n",
      "Input: start_ the kitchen table was bare except for a bowl of fruit _end\n",
      "Predicted translation: der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der \n",
      "Actual translation: start_ abgesehen von einer schale obst war der kuechentisch leer _end \n",
      "None\n",
      "Input: start_ men arent so different from women _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ maenner unterscheiden sich gar nicht so sehr von frauen _end \n",
      "None\n",
      "Input: start_ one of these two answers is right _end\n",
      "Predicted translation: tom ist der tuer _end \n",
      "Actual translation: start_ eine dieser beiden antworten ist korrekt _end \n",
      "None\n",
      "Input: start_ weve known each other for a long time _end\n",
      "Predicted translation: tom ist der nacht bin _end \n",
      "Actual translation: start_ wir kennen uns schon sehr lange _end \n",
      "None\n",
      "Input: start_ maybe we could study together in the library _end\n",
      "Predicted translation: ich habe eine gute idee _end \n",
      "Actual translation: start_ vielleicht koennten wir in der bibliothek zusammen lernen _end \n",
      "None\n",
      "Input: start_ i shuffled the cards _end\n",
      "Predicted translation: ich trinke kaffee _end \n",
      "Actual translation: start_ ich mischte die karten _end \n",
      "None\n",
      "Input: start_ tom went shopping with his girlfriend _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ tom ging mit seiner freundin einkaufen _end \n",
      "None\n",
      "Input: start_ tom made a fool of himself in front of mary _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ tom hat sich vor maria laecherlich gemacht _end \n",
      "None\n",
      "Input: start_ im important _end\n",
      "Predicted translation: ich habe eine gute idee _end \n",
      "Actual translation: start_ ich bin wichtig _end \n",
      "None\n",
      "Input: start_ he did what his conscience dictated _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ er tat was sein gewissen ihm gebot _end \n",
      "None\n",
      "Input: start_ you arent rich are you _end\n",
      "Predicted translation: das ist sehr gut _end \n",
      "Actual translation: start_ sie sind nicht reich oder _end \n",
      "None\n",
      "Input: start_ tom wasnt really busy _end\n",
      "Predicted translation: tom _end \n",
      "Actual translation: start_ tom war nicht sehr beschaeftigt _end \n",
      "None\n",
      "Input: start_ i like it when tom does that _end\n",
      "Predicted translation: ich habe _end \n",
      "Actual translation: start_ ich mag es wenn tom das macht _end \n",
      "None\n",
      "Input: start_ if you trust such a fellow youll lose everything you have _end\n",
      "Predicted translation: was du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist \n",
      "Actual translation: start_ wenn sie sich so einem kerl anvertrauen verlieren sie alles was sie haben _end \n",
      "None\n",
      "Input: start_ tom teaches french to his friends _end\n",
      "Predicted translation: tom ist nicht sehr _end \n",
      "Actual translation: start_ tom unterrichtet seine freunde in franzoesisch _end \n",
      "None\n",
      "Input: start_ youve taken a long time eating lunch _end\n",
      "Predicted translation: was du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist \n",
      "Actual translation: start_ du hast dir viel zeit beim mittagessen gelassen _end \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
