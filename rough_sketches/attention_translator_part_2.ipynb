{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data source: http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, RNN, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "import regex as re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import spacy\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "# disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: x.lower())\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "# Set of all special characters\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# Remove all the special characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "df_en_de['german']=df_en_de['german'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209317\n",
      "20932\n"
     ]
    }
   ],
   "source": [
    "max_len = 10\n",
    "\n",
    "pairs = df_en_de\n",
    "pairs['english_length'] = pairs['english'].apply(lambda x: len(x.split(' ')))\n",
    "pairs['german_length'] = pairs['german'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "pairs = pairs[pairs['english_length'] <= max_len]\n",
    "pairs = pairs[pairs['german_length'] <= max_len]\n",
    "print(len(pairs))\n",
    "pairs = pairs.sample(frac = 0.1)\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_en_words=set()\n",
    "for eng in pairs['english']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_en_words:\n",
    "            all_en_words.add(word)\n",
    "\n",
    "# Vocabulary of German \n",
    "all_de_words=set()\n",
    "for de in pairs['german']:\n",
    "    for word in de.split():\n",
    "        if word not in all_de_words:\n",
    "            all_de_words.add(word)\n",
    "\n",
    "# Max Length of source sequence\n",
    "length_list=[]\n",
    "for l in pairs['english']:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(length_list)\n",
    "\n",
    "# Max Length of target sequence\n",
    "length_list=[]\n",
    "for l in pairs['german']:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(length_list)\n",
    "\n",
    "\n",
    "input_words = sorted(list(all_en_words))\n",
    "target_words = sorted(list(all_de_words))\n",
    "\n",
    "# Calculate Vocab size for both source and target\n",
    "# Add 1 for zero padding\n",
    "num_encoder_tokens = len(all_en_words) + 1\n",
    "num_decoder_tokens = len(all_de_words) + 1\n",
    "\n",
    "\n",
    "\n",
    "# Create word to token dictionary for both source and target\n",
    "#input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "#target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "input_word_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_word_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "# Create token to word dictionary for both source and target\n",
    "# reverse_input_token_index = dict((i, word) for word, i in input_token_index.items())\n",
    "# reverse_target_token_index = dict((i, word) for word, i in target_token_index.items())\n",
    "input_index_word = dict((i, word) for word, i in input_word_index.items())\n",
    "target_index_word = dict((i, word) for word, i in target_word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pairs['english'], pairs['german']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):          # j = batch number\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32') # (m, max_len)\n",
    "            \n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32') # (m, max_len)\n",
    "\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')    # (m, max_len, num_decoder_tokens)\n",
    "            \n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_word_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_word_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_word_index[word]] = 1. \n",
    "                        \"\"\" This should be target_token_index[word] - 1\"\"\"\n",
    "            # decoder_target_data = np.transpose(decoder_target_data, axes = [1, 0, 2])\n",
    "            # decoder_target_data = list(decoder_target_data)\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1838"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_batch = generate_batch()\n",
    "np.argmax(next(iter_batch)[1][:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = max_len\n",
    "Ty = max_len\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis = -1)\n",
    "densor1 = Dense(10, activation = 'tanh')\n",
    "densor2 = Dense (1, activation = 'relu')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention (h, s_prev):\n",
    "\n",
    "    # calculate the Context vector for one time-step of decoder\n",
    "\n",
    "    # h = (m, Tx, n_h)\n",
    "    # s_prev = (m, n_s)\n",
    "    # returns: context – we will then use [context; y_prev] as input of Decoder\n",
    "\n",
    "    s_prev = repeator(s_prev)                   # (m, Tx, n_s)\n",
    "    concat = concatenator([h, s_prev])          # (m, Tx, n_h + n_s)\n",
    "    e = densor1 (concat)                        # (m, Tx, 10)\n",
    "    energies = densor2 (e)                      # (m, Tx, 1)\n",
    "    alphas = tf.nn.softmax(energies, axis = 1)  # (m, Tx, 1)\n",
    "    context = dotor([alphas, h])                # alphas = (m, Tx, 1)\n",
    "                                                # h = (m, Tx, n_h)\n",
    "                                                # (m, 1, n_h)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb_dim = 300\n",
    "y_emb_dim = 300\n",
    "\n",
    "n_h = 200\n",
    "n_s = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 11:10:22.114934: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "post_attention_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(num_decoder_tokens, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10250 (None, 10, 10250)\n"
     ]
    }
   ],
   "source": [
    "# h = hidden state of pre-attention RNN layer\n",
    "# s = hidden state of post-attention RNN layer\n",
    "\n",
    "x_inputs = Input(shape = (Tx,))                         # (None, Tx) = (m, Tx)\n",
    "x_emb_layer = Embedding(\n",
    "                            num_encoder_tokens, \n",
    "                            x_emb_dim, \n",
    "                            mask_zero = True\n",
    "                            )      \n",
    "x_emb = x_emb_layer(x_inputs)                         # (None, Tx, x_emb_dim) = (m, Tx, x_emb_dim)\n",
    "\n",
    "y_inputs = Input(shape = (Ty,))                         # (None, Ty) = (m, Ty)\n",
    "y_emb_layer = Embedding(\n",
    "                          num_decoder_tokens,\n",
    "                          y_emb_dim,\n",
    "                          mask_zero = True\n",
    "                          )\n",
    "y_emb = y_emb_layer(y_inputs)                         # (None, Ty, y_emb_dim) = (m, Ty, y_emb_dim)\n",
    "\n",
    "\n",
    "# pass x embeddings through pre-attention LSTM layer\n",
    "# here, we will use the final hidden-state as the initial post-attention LSTM hidden state\n",
    "\n",
    "enc_lstm_layer = LSTM(n_h, return_sequences=True, return_state = True)\n",
    "h_enc, s_enc, c_enc = enc_lstm_layer(x_emb)                             # h_enc = (None, Tx, n_h) = (m, Tx, n_h)\n",
    "                                                                        # s_enc = (None, n_h) = (m, n_h)\n",
    "                                                                        # c_enc = (None, n_h) = (m, n_h)\n",
    "# s<0> and c<0> for decoder = s<ty> and c<ty> for encoder\n",
    "s_dec = s_enc\n",
    "c_dec = c_enc                                                              \n",
    "\n",
    "outputs = []\n",
    "for t in range(Ty):\n",
    "    context = one_step_attention(h_enc, s_dec)                                            # context = (m, 1, n_h)\n",
    "    concat = Concatenate(axis = -1)([context, tf.expand_dims(y_emb[:,t,:],1)])            # concat = (m, 1, n_h + y_emb_dim)\n",
    "    \n",
    "    # update decoder LSTM hidden state (s) and cell state (c)\n",
    "    _, s_dec, c_dec = post_attention_LSTM_cell (initial_state = [s_dec, c_dec], inputs = concat)        # s = (None, Ty, n_s)\n",
    "    \n",
    "    # pass decoder LSTM hidden state (s) through output layer to get y prediction\n",
    "    out = output_layer(s_dec)                                                                   # out = (m, num_decoder_tokens)\n",
    "    outputs.append(out)\n",
    "                                                            \n",
    "outputs = tf.stack(outputs, axis = 1)\n",
    "print(num_decoder_tokens, outputs.shape)\n",
    "model = Model(inputs = [x_inputs, y_inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= 'Adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train) # Total Training samples\n",
    "val_samples = len(X_test) # total validation samples\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "130/130 [==============================] - 37s 284ms/step - loss: 3.3899 - acc: 0.1473 - val_loss: 3.4142 - val_acc: 0.1528\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 37s 282ms/step - loss: 3.1734 - acc: 0.1627 - val_loss: 3.2692 - val_acc: 0.1651\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 37s 282ms/step - loss: 3.0050 - acc: 0.1791 - val_loss: 3.1606 - val_acc: 0.1849\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 38s 294ms/step - loss: 2.8388 - acc: 0.2028 - val_loss: 3.0348 - val_acc: 0.2028\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 37s 282ms/step - loss: 2.6812 - acc: 0.2191 - val_loss: 2.9283 - val_acc: 0.2144\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = 5,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "130/130 [==============================] - 40s 306ms/step - loss: 2.5361 - acc: 0.2353 - val_loss: 2.8456 - val_acc: 0.2261\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 37s 283ms/step - loss: 2.3968 - acc: 0.2492 - val_loss: 2.7756 - val_acc: 0.2335\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 37s 282ms/step - loss: 2.2774 - acc: 0.2615 - val_loss: 2.7124 - val_acc: 0.2414\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 37s 282ms/step - loss: 2.1659 - acc: 0.2736 - val_loss: 2.6442 - val_acc: 0.2515\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 37s 288ms/step - loss: 2.0581 - acc: 0.2865 - val_loss: 2.6051 - val_acc: 0.2559\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = 5,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "130/130 [==============================] - 37s 282ms/step - loss: 1.9546 - acc: 0.2989 - val_loss: 2.5614 - val_acc: 0.2643\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 37s 282ms/step - loss: 1.8535 - acc: 0.3095 - val_loss: 2.5207 - val_acc: 0.2670\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 37s 281ms/step - loss: 1.7618 - acc: 0.3203 - val_loss: 2.5032 - val_acc: 0.2707\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 37s 282ms/step - loss: 1.6694 - acc: 0.3314 - val_loss: 2.4593 - val_acc: 0.2786\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 37s 282ms/step - loss: 1.5791 - acc: 0.3428 - val_loss: 2.4483 - val_acc: 0.2786\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = 5,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "130/130 [==============================] - 36s 279ms/step - loss: 1.4887 - acc: 0.3554 - val_loss: 2.4162 - val_acc: 0.2839\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 37s 283ms/step - loss: 1.3965 - acc: 0.3679 - val_loss: 2.4109 - val_acc: 0.2845\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 37s 283ms/step - loss: 1.3159 - acc: 0.3795 - val_loss: 2.3868 - val_acc: 0.2889\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 37s 288ms/step - loss: 1.2395 - acc: 0.3915 - val_loss: 2.3802 - val_acc: 0.2940\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 37s 285ms/step - loss: 1.1753 - acc: 0.4034 - val_loss: 2.3503 - val_acc: 0.2979\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = 5,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "130/130 [==============================] - 39s 299ms/step - loss: 1.1028 - acc: 0.4171 - val_loss: 2.3470 - val_acc: 0.2995\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 37s 284ms/step - loss: 1.0256 - acc: 0.4334 - val_loss: 2.3442 - val_acc: 0.2983\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 37s 285ms/step - loss: 0.9629 - acc: 0.4470 - val_loss: 2.3531 - val_acc: 0.2981\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 37s 286ms/step - loss: 0.9062 - acc: 0.4588 - val_loss: 2.3538 - val_acc: 0.2979\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 37s 286ms/step - loss: 0.8497 - acc: 0.4708 - val_loss: 2.3230 - val_acc: 0.3080\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = 5,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "130/130 [==============================] - 38s 290ms/step - loss: 0.7924 - acc: 0.4859 - val_loss: 2.3149 - val_acc: 0.3083\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 40s 305ms/step - loss: 0.7247 - acc: 0.5009 - val_loss: 2.3226 - val_acc: 0.3097\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 38s 292ms/step - loss: 0.6726 - acc: 0.5118 - val_loss: 2.3194 - val_acc: 0.3095\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 38s 293ms/step - loss: 0.6279 - acc: 0.5213 - val_loss: 2.3246 - val_acc: 0.3112\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 39s 296ms/step - loss: 0.5935 - acc: 0.5284 - val_loss: 2.3532 - val_acc: 0.3066\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = 5,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "130/130 [==============================] - 37s 286ms/step - loss: 0.5676 - acc: 0.5331 - val_loss: 2.3548 - val_acc: 0.3072\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 37s 286ms/step - loss: 0.5179 - acc: 0.5441 - val_loss: 2.3408 - val_acc: 0.3109\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 37s 284ms/step - loss: 0.4761 - acc: 0.5530 - val_loss: 2.3486 - val_acc: 0.3135\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 37s 287ms/step - loss: 0.4362 - acc: 0.5617 - val_loss: 2.3690 - val_acc: 0.3132\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 38s 295ms/step - loss: 0.4013 - acc: 0.5697 - val_loss: 2.3602 - val_acc: 0.3129\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = 5,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_inputs defined above as Input\n",
    "# h_enc, s_enc, c_enc also defined above\n",
    "encoder_model = Model(inputs = x_inputs, outputs = [h_enc, s_enc, c_enc])\n",
    "\n",
    "# x_inputs = (None, Tx)\n",
    "\n",
    "# h_enc = (None, Tx, n_h)\n",
    "# s_enc = (None, n_s) \n",
    "# c_enc = (None, n_s) \n",
    "\n",
    "# The below three decoder inputs will come from encoder_model.predict()\n",
    "decoder_input_h = Input(shape = (Tx, n_h))               # (None, Tx, n_h) \n",
    "decoder_input_s = Input(shape=(n_s, ))                    # (None, n_s) \n",
    "decoder_input_c = Input(shape = (n_s,))                   # (None, n_s) \n",
    "\n",
    "# y_emb_2 will be our y_pred at t-1\n",
    "y_inp_2 = Input(shape = (None,))                        # (None, None) = (m, Ty)\n",
    "y_emb_2 = y_emb_layer(y_inp_2)                          # (None, None, y_emb_dim) = (m, Ty, y_emb_dim)\n",
    "\n",
    "# Use decoder_input_s and decoder_input_h to compute context vector\n",
    "context = one_step_attention(decoder_input_h, decoder_input_s)    # (m, 1, n_h)\n",
    "\n",
    "# concatenate context with y_emb_2\n",
    "concat2 = Concatenate(axis = -1)([context, tf.expand_dims(y_emb_2[:,-1,:],1)])                       \n",
    "                                                        # concat2 = (None, 1, n_h + y_emb_dim)\n",
    "\n",
    "# Feed concat2 as input; decoder_input_s and decoder_input_c as initial state\n",
    "_, decoder_output_s, decoder_output_c = post_attention_LSTM_cell (\n",
    "                                                        initial_state = [decoder_input_s, decoder_input_c], \n",
    "                                                        inputs = concat2\n",
    "                                                        )     \n",
    "                                            # decoder_output_s = (None, n_s) \n",
    "                                            # decoder_output_c = (None, n_s) \n",
    "\n",
    "#decoder_output_s = tf.expand_dims(decoder_output_s, 1)      # decoder_output_s = (None, 1, n_s)\n",
    "#decoder_output_c = tf.expand_dims(decoder_output_c, 1)      # decoder_output_c = (None, 1, n_s)\n",
    "decoder_output_y = output_layer(tf.expand_dims(decoder_output_s,1))           # (None, 1, num_decoder_tokens)\n",
    "\n",
    "decoder_model = Model(inputs = [decoder_input_h, decoder_input_s, decoder_input_c, y_inp_2],\n",
    "                         outputs = [decoder_output_y, decoder_output_s, decoder_output_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4255. 6444. 4253.    0.    0.    0.    0.    0.    0.    0.]] (1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('poets write poems', 'dichter')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder_model -> inputs: x_inputs = (None, Tx)\n",
    "#                  outputs: [h_enc, s_enc, c_enc]\n",
    "# decoder_model –> inputs: decoder_input_h, decoder_input_s, decoder_input_c, y_inp_2\n",
    "                #  outputs: decoder_output_y, decoder_output_s, decoder_output_c\n",
    "\n",
    "sen1 = X_train.iloc[0]\n",
    "seq1 = np.zeros((1, 10))\n",
    "for i, word in enumerate(sen1.split()):\n",
    "    seq1[0,i] = input_word_index[word]\n",
    "print(seq1, seq1.shape)\n",
    "aaa, bbb, ccc = encoder_model.predict(seq1)\n",
    "target_y = np.zeros((1,1))\n",
    "target_y[0,0] = target_word_index['START_']\n",
    "decoder_outs = decoder_model.predict([aaa, bbb, ccc ,target_y])\n",
    "idx = np.argmax(decoder_outs[0])\n",
    "sen1, target_index_word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_sequence):\n",
    "\n",
    "    # input_sequence = (1, max_len)\n",
    "    \n",
    "    # get hidden states + final hidden state + final cell state from encoder \n",
    "    h_enc_pred, s_enc_pred, c_enc_pred = encoder_model.predict(input_sequence)\n",
    "    # print(f's_enc_pred: {s_enc_pred.shape}')\n",
    "    # define y_pred at time 0    \n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0,0] = target_word_index['START_']\n",
    "\n",
    "    #decoder_y_pred = np.zeros((1,1,num_decoder_tokens))\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # initialise hidden state and cell state input for decoder\n",
    "    decoder_s_pred = s_enc_pred                         # (None, n_h) = (m, n_h)\n",
    "    decoder_c_pred = c_enc_pred                         # (None, n_h) = (m, n_h)\n",
    "    \n",
    "    #print('h_enc_pred, decoder_s_pred, decoder_c_pred, target_seq:')\n",
    "    #print(h_enc_pred.shape, decoder_s_pred.shape, decoder_c_pred.shape, target_seq.shape)\n",
    "    count = 0\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_y_pred, decoder_s_pred, decoder_c_pred = decoder_model.predict([h_enc_pred, decoder_s_pred, decoder_c_pred, target_seq])\n",
    "        #print(f'decoder_s_pred: {decoder_s_pred.shape}')   \n",
    "        y_index = np.argmax(decoder_y_pred[0,-1,:])\n",
    "        #print(y_index)\n",
    "        y_word = target_index_word[y_index]\n",
    "        decoded_sentence += ' ' + y_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (y_word == '_END' or\n",
    "           len(decoded_sentence.split()) > max_len):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = y_index\n",
    "        count += 1\n",
    "        #print(f'count: {count}')\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dichter schreiben gedichte _END'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(seq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence):\n",
    "    \"\"\"\n",
    "    sentence = string\n",
    "    \"\"\"\n",
    "    \n",
    "    encoder_input_data = np.zeros((1, max_len))     # (1, max_len)\n",
    "    \n",
    "    sentence = sentence.lower().split()\n",
    "    #print(sentence)\n",
    "    for j, word in enumerate(sentence):\n",
    "        encoder_input_data[0,j] = input_word_index[word]        # (1, max_len)\n",
    "    # print(encoder_input_data.shape)\n",
    "    return encoder_input_data                           # (1, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('poets write poems', ' lass den tisch gegessen _END')\n",
      "('he admired my new car', ' er sieht mein auto hat _END')\n",
      "('tom was offended', ' tom war nervös _END')\n",
      "('tom is a friend of a friend of mine', ' tom ist ein bisschen ein guter freund _END')\n",
      "('plutonium239 has a halflife of 24100 years', ' plutonium239 hat eine hohe von des gegessen _END')\n",
      "('i think tom is drunk', ' ich glaube tom ist gerade glücklich _END')\n",
      "('cardboard is stronger than paper', ' pappe ist schwerer als meine mutter _END')\n",
      "('a good idea occurred to him', ' ein buch ist von ihm sehr geholfen _END')\n",
      "('tell me a joke', ' gib mir ein hund _END')\n",
      "('i just want to sleep', ' ich will nur im bett _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "1740\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "4554\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "74\n",
      "count: 3\n",
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "2377\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "3998\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "2109\n",
      "count: 3\n",
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "8224\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "4554\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "8224\n",
      "count: 3\n",
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "8224\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "4554\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "2098\n",
      "count: 3\n",
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "4433\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "3928\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "1845\n",
      "count: 3\n",
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "4433\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "3928\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "8224\n",
      "count: 3\n",
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "1740\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "4554\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "2098\n",
      "count: 3\n",
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "1845\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "4356\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "4554\n",
      "count: 3\n",
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "4433\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "3928\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "5755\n",
      "count: 3\n",
      "s_enc_pred: (1, 200)\n",
      "decoder_s_pred: (1, 200)\n",
      "4433\n",
      "count: 1\n",
      "decoder_s_pred: (1, 200)\n",
      "3928\n",
      "count: 2\n",
      "decoder_s_pred: (1, 200)\n",
      "6109\n",
      "count: 3\n",
      "('poets write poems', ' das ist _END')\n",
      "('he admired my new car', ' er hat eine')\n",
      "('tom was offended', ' tom ist tom')\n",
      "('tom is a friend of a friend of mine', ' tom ist ein')\n",
      "('plutonium239 has a halflife of 24100 years', ' ich habe die')\n",
      "('i think tom is drunk', ' ich habe tom')\n",
      "('cardboard is stronger than paper', ' das ist ein')\n",
      "('a good idea occurred to him', ' die hund ist')\n",
      "('tell me a joke', ' ich habe mich')\n",
      "('i just want to sleep', ' ich habe nicht')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "('poets write poems', ' lass den tisch _END')\n",
      "('he admired my new car', ' er hat sich sehr gut zu _END')\n",
      "('tom was offended', ' tom war ziemlich _END')\n",
      "('tom is a friend of a friend of mine', ' tom ist ein ein guter guter guter freund _END')\n",
      "('plutonium239 has a halflife of 24100 years', ' gib mir eine paar minuten _END')\n",
      "('i think tom is drunk', ' ich habe tom das geld _END')\n",
      "('cardboard is stronger than paper', ' ist die tür aus _END')\n",
      "('a good idea occurred to him', ' ein hund ist sehr gut _END')\n",
      "('tell me a joke', ' darf ich mich ein bisschen _END')\n",
      "('i just want to sleep', ' ich möchte dass du etwas _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "s_enc_pred: (1, 200)\n",
      "('poets write poems', ' lass den tisch _END')\n",
      "('he admired my new car', ' er hat mein auto gestohlen zu _END')\n",
      "('tom was offended', ' tom war nervös _END')\n",
      "('tom is a friend of a friend of mine', ' tom ist ein ein guter freund für ein neues mädchen _END')\n",
      "('plutonium239 has a halflife of 24100 years', ' gib mir eine stück von den tisch _END')\n",
      "('i think tom is drunk', ' ich glaube tom ist ziemlich langsam _END')\n",
      "('cardboard is stronger than paper', ' schau nur ein wenig wert _END')\n",
      "('a good idea occurred to him', ' ein buch ist sehr stolz als tom _END')\n",
      "('tell me a joke', ' gib mir mir ein hund _END')\n",
      "('i just want to sleep', ' ich möchte dass du etwas gehen _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('poets write poems', ' lass den tisch gegessen _END')\n",
      "('he admired my new car', ' er sieht mein auto hat _END')\n",
      "('tom was offended', ' tom war nervös _END')\n",
      "('tom is a friend of a friend of mine', ' tom ist ein bisschen ein guter freund _END')\n",
      "('plutonium239 has a halflife of 24100 years', ' plutonium239 hat eine hohe von des gegessen _END')\n",
      "('i think tom is drunk', ' ich glaube tom ist gerade glücklich _END')\n",
      "('cardboard is stronger than paper', ' pappe ist schwerer als meine mutter _END')\n",
      "('a good idea occurred to him', ' ein buch ist von ihm sehr geholfen _END')\n",
      "('tell me a joke', ' gib mir ein hund _END')\n",
      "('i just want to sleep', ' ich will nur im bett _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('poets write poems', ' dichter gehen sie _END')\n",
      "('he admired my new car', ' er bewunderte mein auto _END')\n",
      "('tom was offended', ' tom war beleidigt _END')\n",
      "('tom is a friend of a friend of mine', ' tom ist ein bisschen von meiner freundin _END')\n",
      "('plutonium239 has a halflife of 24100 years', ' plutonium239 hat eine hohe von 24100 jahren _END')\n",
      "('i think tom is drunk', ' ich glaube tom ist ziemlich langsam _END')\n",
      "('cardboard is stronger than paper', ' pappe ist fester als papier _END')\n",
      "('a good idea occurred to him', ' ein buch ist von ihm nicht gut _END')\n",
      "('tell me a joke', ' gib mir bitte einen hund _END')\n",
      "('i just want to sleep', ' ich will nur etwas schlafen _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('shes thirtythree', ' sie ist _END')\n",
      "('it wasnt funny at all', ' das war immer noch nicht lustig _END')\n",
      "('that black bird is not a blackbird', ' diese maschine ist nicht mehr wert _END')\n",
      "('does tom want coffee', ' trinkt tom einen kaffee _END')\n",
      "('our visitors are at the door', ' unsere tür sind auf dem fenster _END')\n",
      "('mary likes japan doesnt she', ' maria sieht nicht mehr als _END')\n",
      "('you can go to the bus station', ' du musst mit dem abendessen gehen _END')\n",
      "('our plans are taking shape', ' unsere familienehre ist unsere parks _END')\n",
      "('can i use your telephone', ' kann ich gerne hören _END')\n",
      "('id like to help you', ' ich möchte gerne ihnen helfen _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_test.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('poets write poems', ' dichter schreiben gedichte _END')\n",
      "('he admired my new car', ' er bewunderte mein neues auto _END')\n",
      "('tom was offended', ' tom war beleidigt _END')\n",
      "('tom is a friend of a friend of mine', ' tom ist ein freund von mir freund ein _END')\n",
      "('plutonium239 has a halflife of 24100 years', ' plutonium239 hat eine halbwertszeit von 24100 jahren _END')\n",
      "('i think tom is drunk', ' ich glaube tom hat betrunken _END')\n",
      "('cardboard is stronger than paper', ' pappe ist fester als papier _END')\n",
      "('a good idea occurred to him', ' ein guter film ist ihm nicht schlecht _END')\n",
      "('tell me a joke', ' erzähle mir einen witz _END')\n",
      "('i just want to sleep', ' ich will einfach warten _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: x.lower())\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "# Set of all special characters\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# Remove all the special characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "df_en_de['german']=df_en_de['german'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x : '<start> '+ x + ' <end>')\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x : '<start> '+ x + ' <end>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename dataframe for convenience\n",
    "pairs = df_en_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199379\n",
      "19938\n"
     ]
    }
   ],
   "source": [
    "max_len = 10\n",
    "\n",
    "pairs = df_en_de\n",
    "pairs['english_length'] = pairs['english'].apply(lambda x: len(x.split(' ')))\n",
    "pairs['german_length'] = pairs['german'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "pairs = pairs[pairs['english_length'] <= max_len]\n",
    "pairs = pairs[pairs['german_length'] <= max_len]\n",
    "print(len(pairs))\n",
    "pairs = pairs.sample(frac = 0.1)\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "      <th>english_length</th>\n",
       "      <th>german_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78158</th>\n",
       "      <td>&lt;start&gt; two plus two makes four &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; zwei plus zwei ist vier &lt;end&gt;</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28780</th>\n",
       "      <td>&lt;start&gt; you cant fire me &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; sie können mir nicht kündigen &lt;end&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35875</th>\n",
       "      <td>&lt;start&gt; why dont you stop &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; warum hältst du nicht an &lt;end&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     english  \\\n",
       "78158  <start> two plus two makes four <end>   \n",
       "28780         <start> you cant fire me <end>   \n",
       "35875        <start> why dont you stop <end>   \n",
       "\n",
       "                                            german  english_length  \\\n",
       "78158        <start> zwei plus zwei ist vier <end>               7   \n",
       "28780  <start> sie können mir nicht kündigen <end>               6   \n",
       "35875       <start> warum hältst du nicht an <end>               6   \n",
       "\n",
       "       german_length  \n",
       "78158              7  \n",
       "28780              7  \n",
       "35875              7  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = list(pairs['english'])\n",
    "german_text = list(pairs['german'])\n",
    "both = list(map(list, zip(english_text, german_text)))\n",
    "cleaned_pairs = both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<start> two plus two makes four <end>',\n",
       "  '<start> zwei plus zwei ist vier <end>'],\n",
       " ['<start> you cant fire me <end>',\n",
       "  '<start> sie können mir nicht kündigen <end>'],\n",
       " ['<start> why dont you stop <end>', '<start> warum hältst du nicht an <end>'],\n",
       " ['<start> where did you get all this from <end>',\n",
       "  '<start> woher hast du das alles <end>'],\n",
       " ['<start> were you in boston last week <end>',\n",
       "  '<start> warst du letzte woche in boston <end>']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()                 # Run the function when you initialise class\n",
    "                                            # So when you initialise the class, \n",
    "                                            # the variable has word2idx and idx2word dictionaries already\n",
    "\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "\n",
    "# Function to calculate maximum length of the sequence\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(pairs, num_examples):\n",
    "\n",
    "    inp_lang = LanguageIndex(en for en, de in cleaned_pairs)\n",
    "    targ_lang = LanguageIndex(de for en, de in cleaned_pairs)\n",
    "\n",
    "    # English sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, de in cleaned_pairs]\n",
    "    # German sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in de.split(' ')] for en, de in cleaned_pairs]\n",
    "\n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    # First argument of pad_sequences = list of sequences, where each sequence is a list of integers\n",
    "    # second argument = number of integers per sequence\n",
    "    # make sure to set \"padding\" = \"post\" to append zeros at end and not beginning\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                    maxlen=max_length_inp,\n",
    "                                                                    padding='post')\n",
    "\n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                    maxlen=max_length_tar, \n",
    "                                                                    padding='post')\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tensors\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(cleaned_pairs, len(cleaned_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "# each training set is 2D numpy arary\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17944, 10), (17944, 10))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train.shape, target_tensor_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  62, 1758, 9037, 6570, 1658,   61,    0,    0,    0,    0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train[0]\n",
    "target_tensor_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 08:26:07.286937: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters of the model\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "# Create batch generator to be used by modle to load data in batches\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (280, 2, 64, 10)\n"
     ]
    }
   ],
   "source": [
    "train_np = np.stack(list(dataset))\n",
    "print(type(train_np), train_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  \n",
    "    return tf.keras.layers.GRU(units, \n",
    "                                return_sequences=True, \n",
    "                                return_state=True, \n",
    "                                recurrent_activation='sigmoid',             # recurrent_activation refers to the \"update gate\"\n",
    "                                recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        # this is the step 1 described in the blog to compute scores s1, s2, ...\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # this is the step 2 described in the blog to compute attention weights e1, e2, ...\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # this is the step 3 described in the blog to compute the context_vector = e1*h1 + e2*h2 + ...\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # this is the step 4 described in the blog to concatenate the context vector with the output of the previous time step\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        # this is the step 5 in the blog, to compute the next output word in the sequence\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        # return current output, current state and the attention weights\n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create objects of Class Encoder and Class Decoder\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
