{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNmRO1UCiTGL"
      },
      "source": [
        "- functioning NMT model\n",
        "- attention model using classes\n",
        "- bi-directional LSTMs in Encoder\n",
        "- training worked; loss went down over 25 epochs\n",
        "- No attention plots or embeddings yet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kn6y0Y2eiTGO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Bidirectional, Concatenate, LSTM, Embedding, Dense\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "\n",
        "#import plotly.graph_objects as go\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oaqyk5z-iTGP",
        "outputId": "42098d6d-7bc2-4671-a909-10a59a8a6a3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FczjOYETiTGQ"
      },
      "outputs": [],
      "source": [
        "#df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])\n",
        "df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6XuDurryiTGQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(\"'\", '', sentence)\n",
        "    sentence = sentence.replace('ü', 'ue').replace('ä', 'ae').replace('ö', 'oe').replace('ß', 'ss')\n",
        "    exclude = set(string.punctuation)\n",
        "    sentence = ''.join(ch for ch in sentence if ch not in exclude)\n",
        "    sentence = 'start_ ' + sentence + ' _end'\n",
        "    sentence = sentence.encode(\"ascii\", \"ignore\")\n",
        "    sentence = sentence.decode()\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G3QynPgBiTGR"
      },
      "outputs": [],
      "source": [
        "pairs = df_en_de\n",
        "pairs = pairs.sample(frac = 0.1)\n",
        "pairs['english'] = pairs['english'].apply(preprocess_sentence)\n",
        "pairs['german'] = pairs['german'].apply(preprocess_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1AA7rM5ziTGR"
      },
      "outputs": [],
      "source": [
        "source = pairs['english']\n",
        "target = pairs ['german']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t5MIVnn2iTGS"
      },
      "outputs": [],
      "source": [
        "# create tokenizer & tensor for source and target\n",
        "source_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_sentence_tokenizer.fit_on_texts(source)\n",
        "source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n",
        "source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor, padding='post' )\n",
        "\n",
        "target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_sentence_tokenizer.fit_on_texts(target)\n",
        "target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n",
        "target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nPg2iNB0iTGS"
      },
      "outputs": [],
      "source": [
        "source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor = train_test_split(\n",
        "                                                                source_tensor, target_tensor,test_size=0.2\n",
        "                                                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6Op5cOgyiTGT"
      },
      "outputs": [],
      "source": [
        "max_target_length= max(len(t) for t in  target_tensor)\n",
        "max_source_length= max(len(t) for t in source_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK2LFnW7iTGU",
        "outputId": "473848b0-282b-4f18-ed67-5d57077a1319"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(49, 43)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_target_length, max_source_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "igd_hwb9iTGU"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "#Create data in memeory \n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BATCH_SIZE)\n",
        "# shuffles the data in the batch\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtr_qVkriTGV",
        "outputId": "618316c2-c34b-4ea0-b87a-faf58160c05f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 43) (32, 49)\n",
            "tf.Tensor(\n",
            "[   1    3   57  324   29 1238    2    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0], shape=(43,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "source_batch, target_batch =next(iter(dataset))\n",
        "print(source_batch.shape, target_batch.shape)\n",
        "print(source_batch[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WQY09zGTiTGV"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(source_train_tensor)\n",
        "steps_per_epoch= BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim= 256\n",
        "units= 256\n",
        "attention_layer_units = 100\n",
        "source_vocab_size= len(source_sentence_tokenizer.word_index)+1\n",
        "target_vocab_size= len(target_sentence_tokenizer.word_index)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6v1mFdcxiTGV"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.encoder_units = encoder_units\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm= Bidirectional(LSTM (encoder_units, \n",
        "                                      return_sequences=True,\n",
        "                                      return_state=True,                                      \n",
        "                                      recurrent_initializer='glorot_uniform'\n",
        "                                     ))\n",
        "    def call(self, x):\n",
        "                                                                # x = (m, Tx)\n",
        "                                                                # hidden = (m, encoder_units)\n",
        "                                                                \n",
        "        x = self.embedding(x)                                   # x = (m, Tx, embedding_dim)\n",
        "        # pass input x through bi-directional LSTM\n",
        "                                                                \n",
        "        (enc_sequential, enc_forward_h, \n",
        "        enc_forward_c, enc_backward_h, enc_backward_c) = self.lstm(x)\n",
        "\n",
        "        # concatenate forward and backward states\n",
        "        enc_final_h = Concatenate()([enc_forward_h, enc_backward_h])\n",
        "        enc_final_c = Concatenate()([enc_forward_c, enc_backward_c])\n",
        "\n",
        "        return enc_sequential, enc_final_h, enc_final_c                     # enc_sequential = (m, Tx, 2 * encoder_units) \n",
        "                                                                            # enc_h = (m, 2 * encoder_units)\n",
        "                                                                            # enc_c = (m, 2 * encoder_units)\n",
        "    \n",
        "    # def initialize_state(self):\n",
        "    #     return tf.zeros((self.batch_size, self.encoder_units))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxvIG1d4iTGW",
        "outputId": "dcd5ecb8-398b-4238-c164-5966436278fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder sequential: (32, 43, 512)\n",
            "Encoder final state_h: (32, 512)\n",
            "Encoder final state_c: (32, 512)\n"
          ]
        }
      ],
      "source": [
        "# check dimensions\n",
        "encoder = Encoder(source_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "#initial_state = encoder.initialize_state()\n",
        "enc_sequential, enc_final_h, enc_final_c = encoder(source_batch)\n",
        "\n",
        "print (f'Encoder sequential: {enc_sequential.shape}')\n",
        "print (f'Encoder final state_h: {enc_final_h.shape}')\n",
        "print (f'Encoder final state_c: {enc_final_c.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M79s6f5miTGW"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super( BahdanauAttention, self).__init__()\n",
        "        self.W1= tf.keras.layers.Dense(units)  # decoder hidden (at time-step \"t-1\")\n",
        "        self.W2= tf.keras.layers.Dense(units)  # encoder hidden (at time-step \"t\")\n",
        "        self.V= tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, dec_hidden, enc_hidden):\n",
        "                                                                # dec_hidden = (m, 2*units) \n",
        "                                                                # enc_hidden:   (m, Tx, 2*units)\n",
        "\n",
        "        dec_hidden_with_time = tf.expand_dims(dec_hidden, 1)    # dec_hidden_with_time = (m, 1, 2*units)\n",
        "        \n",
        "                                                                \n",
        "        # W1() = (m, 1, 10) \n",
        "        # W2() = (m, Tx, 10)\n",
        "        # Broadcasting happens when you add\n",
        "        # W1() + W2 () = (m, Tx, 10)\n",
        "\n",
        "        score = self.V(tf.nn.tanh(self.W1(dec_hidden_with_time) + self.W2(enc_hidden))) # (m, Tx, 1)\n",
        "        \n",
        "        # normalise scores with softmax\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)                                # (m, Tx, 1)\n",
        "        \n",
        "        # apply each weight to encoder hidden state at respective time-step \n",
        "        context_vector= attention_weights * enc_hidden                                  # (m, Tx, 2*units)\n",
        "       \n",
        "        # linear combination of enc_hidden vectors for all Tx\n",
        "        # so sum along Tx axis\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)                          # (m, 2*units)\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3EhD_3tiTGW",
        "outputId": "fc9b275b-9530-4c9d-f14b-71021ce527a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "context vector: (batch size, units) (32, 512)\n",
            "attention weights: (batch_size, sequence_length, 1) (32, 43, 1)\n"
          ]
        }
      ],
      "source": [
        "attention_layer= BahdanauAttention(attention_layer_units)\n",
        "attention_result, attention_weights = attention_layer(enc_final_h, enc_sequential)\n",
        "print(f\"context vector: (batch size, units) {attention_result.shape}\")\n",
        "print(f\"attention weights: (batch_size, sequence_length, 1) {attention_weights.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lMa2eOefiTGX"
      },
      "outputs": [],
      "source": [
        "# Decoder for one time-step\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_sz):\n",
        "        super (Decoder,self).__init__()\n",
        "        self.batch_sz= batch_sz\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm= LSTM (decoder_units, \n",
        "                        return_sequences= True,\n",
        "                        return_state=True,\n",
        "                        recurrent_initializer='glorot_uniform')\n",
        "        # Fully connected layer\n",
        "        self.fc= Dense(vocab_size)      # Note, we don't use an activation here.\n",
        "                                        # For the calculation of the loss, we will use \n",
        "                                        # sparse_softmax_cross_entropy_with_logits, which performs \n",
        "                                        # the softmax on the logits internally for greater efficiency\n",
        "        \n",
        "        # attention\n",
        "        self.attention = BahdanauAttention(attention_layer_units)\n",
        "    \n",
        "    def call(self, y, dec_h, dec_c, enc_sequential):\n",
        "                                                                                    # dec_h: (m, 2*units) \n",
        "                                                                                    # dec_c: (m, 2*units)\n",
        "                                                                                    # enc_sequential: (m, Tx, 2*units) \n",
        "\n",
        "        context_vector, attention_weights = self.attention(dec_h, enc_sequential)   # context_vector = (m, 2*units)\n",
        "        \n",
        "        y= self.embedding(y)                                                        # y = (m, 1, embedding_dim)\n",
        "        \n",
        "        # concatenate context vector and embedding for output sequence\n",
        "        y = tf.concat([tf.expand_dims(context_vector, 1), y],                       # (m, 1, 2*units) + (m, 1, embedding_dim)\n",
        "                                      axis=-1)                                      # (m, 1, 2*units + embedding_dim)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, dec_h, dec_c = self.lstm(y, initial_state = [dec_h, dec_c])                                         # output = (m, 1, 2*units)\n",
        "                                                                                    # dec_h = (m, 2*units)\n",
        "                                                                                    # dec_c = (m, 2*units)\n",
        "\n",
        "        output= tf.reshape(output, (-1, output.shape[2]))                           # output = (m, 2*units)\n",
        "        \n",
        "        # pass the output thru Fc layers\n",
        "        y = self.fc(output)                                                         # y = (m, vocab_size)\n",
        "        return y, dec_h, dec_c, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOp4bRG2iTGX",
        "outputId": "a4b79514-9957-46e6-da08-c0da35006122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (32, 12006)\n"
          ]
        }
      ],
      "source": [
        "# Make sure to pass in \"2*units\", since the encoder uses bi-directional LSTM\n",
        "# We're feeding final_h and final_c from Encoder as init_h and init_c for Decoder\n",
        "decoder= Decoder(target_vocab_size, embedding_dim, 2*units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final_h, enc_final_c, enc_sequential)\n",
        "print (f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yYLjjeqaiTGX"
      },
      "outputs": [],
      "source": [
        "#Define the optimizer and the loss function\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wE2kUz2-iTGX"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):      # \"real\" = (m, 1), \"pred\" = (m, vocab_size)\n",
        "    mask = 1 - np.equal(real, 0)    # mask = 1 when \"real\" != 0\n",
        "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piQhpZnDiTGY",
        "outputId": "3c76c258-1235-488c-f0c2-564f60b39b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.4018\n",
            "Epoch 1 Batch 50 Loss 0.9843\n",
            "Epoch 1 Batch 100 Loss 0.8379\n",
            "Epoch 1 Batch 150 Loss 0.8643\n",
            "Epoch 1 Batch 200 Loss 0.7574\n",
            "Epoch 1 Batch 250 Loss 0.7565\n",
            "Epoch 1 Batch 300 Loss 0.8579\n",
            "Epoch 1 Batch 350 Loss 0.7772\n",
            "Epoch 1 Batch 400 Loss 0.8775\n",
            "Epoch 1 Batch 450 Loss 0.7663\n",
            "Epoch 1 Batch 500 Loss 0.8439\n",
            "Epoch 1 Batch 550 Loss 0.7852\n",
            "Epoch 1 Batch 600 Loss 0.7404\n",
            "Epoch 1 Loss 0.8419\n",
            "Time taken for 1 epoch 571.0552086830139 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.7107\n",
            "Epoch 2 Batch 50 Loss 0.7231\n",
            "Epoch 2 Batch 100 Loss 0.6233\n",
            "Epoch 2 Batch 150 Loss 0.6535\n",
            "Epoch 2 Batch 200 Loss 0.7617\n",
            "Epoch 2 Batch 250 Loss 0.6155\n",
            "Epoch 2 Batch 300 Loss 0.5816\n",
            "Epoch 2 Batch 350 Loss 0.7521\n",
            "Epoch 2 Batch 400 Loss 0.6159\n",
            "Epoch 2 Batch 450 Loss 0.6223\n",
            "Epoch 2 Batch 500 Loss 0.6742\n",
            "Epoch 2 Batch 550 Loss 0.6651\n",
            "Epoch 2 Batch 600 Loss 0.6689\n",
            "Epoch 2 Loss 0.6798\n",
            "Time taken for 1 epoch 561.5900504589081 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.6477\n",
            "Epoch 3 Batch 50 Loss 0.6275\n",
            "Epoch 3 Batch 100 Loss 0.5663\n",
            "Epoch 3 Batch 150 Loss 0.6401\n",
            "Epoch 3 Batch 200 Loss 0.7932\n",
            "Epoch 3 Batch 250 Loss 0.5496\n",
            "Epoch 3 Batch 300 Loss 0.5875\n",
            "Epoch 3 Batch 350 Loss 0.5716\n",
            "Epoch 3 Batch 400 Loss 0.5850\n",
            "Epoch 3 Batch 450 Loss 0.5053\n",
            "Epoch 3 Batch 500 Loss 0.5364\n",
            "Epoch 3 Batch 550 Loss 0.5542\n",
            "Epoch 3 Batch 600 Loss 0.5475\n",
            "Epoch 3 Loss 0.5789\n",
            "Time taken for 1 epoch 553.4694473743439 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5672\n",
            "Epoch 4 Batch 50 Loss 0.5772\n",
            "Epoch 4 Batch 100 Loss 0.4901\n",
            "Epoch 4 Batch 150 Loss 0.5170\n",
            "Epoch 4 Batch 200 Loss 0.5936\n",
            "Epoch 4 Batch 250 Loss 0.4492\n",
            "Epoch 4 Batch 300 Loss 0.4627\n",
            "Epoch 4 Batch 350 Loss 0.4501\n",
            "Epoch 4 Batch 400 Loss 0.4808\n",
            "Epoch 4 Batch 450 Loss 0.4317\n",
            "Epoch 4 Batch 500 Loss 0.5154\n",
            "Epoch 4 Batch 550 Loss 0.5183\n",
            "Epoch 4 Batch 600 Loss 0.4826\n",
            "Epoch 4 Loss 0.4926\n",
            "Time taken for 1 epoch 553.9863555431366 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4957\n",
            "Epoch 5 Batch 50 Loss 0.4250\n",
            "Epoch 5 Batch 100 Loss 0.4358\n",
            "Epoch 5 Batch 150 Loss 0.4363\n",
            "Epoch 5 Batch 200 Loss 0.4564\n",
            "Epoch 5 Batch 250 Loss 0.4150\n",
            "Epoch 5 Batch 300 Loss 0.3979\n",
            "Epoch 5 Batch 350 Loss 0.3821\n",
            "Epoch 5 Batch 400 Loss 0.3699\n",
            "Epoch 5 Batch 450 Loss 0.3311\n",
            "Epoch 5 Batch 500 Loss 0.4153\n",
            "Epoch 5 Batch 550 Loss 0.4518\n",
            "Epoch 5 Batch 600 Loss 0.3544\n",
            "Epoch 5 Loss 0.4153\n",
            "Time taken for 1 epoch 554.3754122257233 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "# initial_state = encoder.initialize_state()\n",
        "# enc_sequential, enc_final_h, enc_final_c = encoder(source_batch, initial_state)\n",
        "# sample_decoder_output, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final_h, enc_final_c, enc_sequential)\n",
        "# decoder returns: y, dec_h, dec_c, attention_weights\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    #initial_state = encoder.initialize_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "                                                        # inp: (batch_size, Tx)\n",
        "                                                        # targ: (batch_size, Ty)\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_sequential, enc_final_h, enc_final_c = encoder(inp)\n",
        "            \n",
        "            dec_h = enc_final_h\n",
        "            dec_c = enc_final_c\n",
        "            \n",
        "            dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)    # (m, 1)   \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_h, dec_c, _ = decoder(dec_input, dec_h, dec_c, enc_sequential) # predictions = (m, vocab_size)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # update dec_input for teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "    \n",
        "    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG3o6stp9vMs",
        "outputId": "a3f9a5d1-152b-4bae-e529-84f2ee8ae283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.4228\n",
            "Epoch 1 Batch 50 Loss 0.4039\n",
            "Epoch 1 Batch 100 Loss 0.3948\n",
            "Epoch 1 Batch 150 Loss 0.3573\n",
            "Epoch 1 Batch 200 Loss 0.3884\n",
            "Epoch 1 Batch 250 Loss 0.3471\n",
            "Epoch 1 Batch 300 Loss 0.3367\n",
            "Epoch 1 Batch 350 Loss 0.3136\n",
            "Epoch 1 Batch 400 Loss 0.3891\n",
            "Epoch 1 Batch 450 Loss 0.2907\n",
            "Epoch 1 Batch 500 Loss 0.3416\n",
            "Epoch 1 Batch 550 Loss 0.3502\n",
            "Epoch 1 Batch 600 Loss 0.3179\n",
            "Epoch 1 Loss 0.3460\n",
            "Time taken for 1 epoch 553.4982507228851 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2916\n",
            "Epoch 2 Batch 50 Loss 0.3165\n",
            "Epoch 2 Batch 100 Loss 0.2944\n",
            "Epoch 2 Batch 150 Loss 0.2923\n",
            "Epoch 2 Batch 200 Loss 0.2704\n",
            "Epoch 2 Batch 250 Loss 0.2812\n",
            "Epoch 2 Batch 300 Loss 0.2997\n",
            "Epoch 2 Batch 350 Loss 0.2561\n",
            "Epoch 2 Batch 400 Loss 0.3098\n",
            "Epoch 2 Batch 450 Loss 0.2252\n",
            "Epoch 2 Batch 500 Loss 0.3036\n",
            "Epoch 2 Batch 550 Loss 0.2497\n",
            "Epoch 2 Batch 600 Loss 0.2668\n",
            "Epoch 2 Loss 0.2869\n",
            "Time taken for 1 epoch 553.5461626052856 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2743\n",
            "Epoch 3 Batch 50 Loss 0.2441\n",
            "Epoch 3 Batch 100 Loss 0.2420\n",
            "Epoch 3 Batch 150 Loss 0.2643\n",
            "Epoch 3 Batch 200 Loss 0.2519\n",
            "Epoch 3 Batch 250 Loss 0.2079\n",
            "Epoch 3 Batch 300 Loss 0.2179\n",
            "Epoch 3 Batch 350 Loss 0.2393\n",
            "Epoch 3 Batch 400 Loss 0.2534\n",
            "Epoch 3 Batch 450 Loss 0.1829\n",
            "Epoch 3 Batch 500 Loss 0.2165\n",
            "Epoch 3 Batch 550 Loss 0.2160\n",
            "Epoch 3 Batch 600 Loss 0.2222\n",
            "Epoch 3 Loss 0.2388\n",
            "Time taken for 1 epoch 549.2550280094147 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2681\n",
            "Epoch 4 Batch 50 Loss 0.2424\n",
            "Epoch 4 Batch 100 Loss 0.2002\n",
            "Epoch 4 Batch 150 Loss 0.2108\n",
            "Epoch 4 Batch 200 Loss 0.2333\n",
            "Epoch 4 Batch 250 Loss 0.1798\n",
            "Epoch 4 Batch 300 Loss 0.2219\n",
            "Epoch 4 Batch 350 Loss 0.1848\n",
            "Epoch 4 Batch 400 Loss 0.1800\n",
            "Epoch 4 Batch 450 Loss 0.1507\n",
            "Epoch 4 Batch 500 Loss 0.2145\n",
            "Epoch 4 Batch 550 Loss 0.2084\n",
            "Epoch 4 Batch 600 Loss 0.1786\n",
            "Epoch 4 Loss 0.1991\n",
            "Time taken for 1 epoch 551.8798279762268 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1808\n",
            "Epoch 5 Batch 50 Loss 0.1973\n",
            "Epoch 5 Batch 100 Loss 0.1682\n",
            "Epoch 5 Batch 150 Loss 0.2173\n",
            "Epoch 5 Batch 200 Loss 0.1629\n",
            "Epoch 5 Batch 250 Loss 0.1489\n",
            "Epoch 5 Batch 300 Loss 0.1893\n",
            "Epoch 5 Batch 350 Loss 0.1777\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "# initial_state = encoder.initialize_state()\n",
        "# enc_sequential, enc_final_h, enc_final_c = encoder(source_batch, initial_state)\n",
        "# sample_decoder_output, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final_h, enc_final_c, enc_sequential)\n",
        "# decoder returns: y, dec_h, dec_c, attention_weights\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    #initial_state = encoder.initialize_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "                                                        # inp: (batch_size, Tx)\n",
        "                                                        # targ: (batch_size, Ty)\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_sequential, enc_final_h, enc_final_c = encoder(inp)\n",
        "            \n",
        "            dec_h = enc_final_h\n",
        "            dec_c = enc_final_c\n",
        "            \n",
        "            dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)    # (m, 1)   \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_h, dec_c, _ = decoder(dec_input, dec_h, dec_c, enc_sequential) # predictions = (m, vocab_size)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # update dec_input for teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "    \n",
        "    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvAuqBYbKKOI",
        "outputId": "61c52677-9e1b-4d8a-f3b7-d93c0236cf19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.1368\n",
            "Epoch 1 Batch 50 Loss 0.1775\n",
            "Epoch 1 Batch 100 Loss 0.1402\n",
            "Epoch 1 Batch 150 Loss 0.1846\n",
            "Epoch 1 Batch 200 Loss 0.1682\n",
            "Epoch 1 Batch 250 Loss 0.1795\n",
            "Epoch 1 Batch 300 Loss 0.1342\n",
            "Epoch 1 Batch 350 Loss 0.1203\n",
            "Epoch 1 Batch 400 Loss 0.1433\n",
            "Epoch 1 Batch 450 Loss 0.1124\n",
            "Epoch 1 Batch 500 Loss 0.1609\n",
            "Epoch 1 Batch 550 Loss 0.1427\n",
            "Epoch 1 Batch 600 Loss 0.1332\n",
            "Epoch 1 Loss 0.1370\n",
            "Time taken for 1 epoch 553.1023693084717 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.1285\n",
            "Epoch 2 Batch 50 Loss 0.1421\n",
            "Epoch 2 Batch 100 Loss 0.1137\n",
            "Epoch 2 Batch 150 Loss 0.1351\n",
            "Epoch 2 Batch 200 Loss 0.1494\n",
            "Epoch 2 Batch 250 Loss 0.0974\n",
            "Epoch 2 Batch 300 Loss 0.1149\n",
            "Epoch 2 Batch 350 Loss 0.1070\n",
            "Epoch 2 Batch 400 Loss 0.1119\n",
            "Epoch 2 Batch 450 Loss 0.0716\n",
            "Epoch 2 Batch 500 Loss 0.1012\n",
            "Epoch 2 Batch 550 Loss 0.1095\n",
            "Epoch 2 Batch 600 Loss 0.0932\n",
            "Epoch 2 Loss 0.1125\n",
            "Time taken for 1 epoch 550.3403558731079 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1127\n",
            "Epoch 3 Batch 50 Loss 0.1101\n",
            "Epoch 3 Batch 100 Loss 0.1056\n",
            "Epoch 3 Batch 150 Loss 0.1076\n",
            "Epoch 3 Batch 200 Loss 0.1268\n",
            "Epoch 3 Batch 250 Loss 0.0910\n",
            "Epoch 3 Batch 300 Loss 0.1160\n",
            "Epoch 3 Batch 350 Loss 0.0766\n",
            "Epoch 3 Batch 400 Loss 0.1163\n",
            "Epoch 3 Batch 450 Loss 0.0646\n",
            "Epoch 3 Batch 500 Loss 0.0935\n",
            "Epoch 3 Batch 550 Loss 0.0798\n",
            "Epoch 3 Batch 600 Loss 0.0864\n",
            "Epoch 3 Loss 0.0915\n",
            "Time taken for 1 epoch 551.9731152057648 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0864\n",
            "Epoch 4 Batch 50 Loss 0.0770\n",
            "Epoch 4 Batch 100 Loss 0.0717\n",
            "Epoch 4 Batch 150 Loss 0.0809\n",
            "Epoch 4 Batch 200 Loss 0.0905\n",
            "Epoch 4 Batch 250 Loss 0.0746\n",
            "Epoch 4 Batch 300 Loss 0.0877\n",
            "Epoch 4 Batch 350 Loss 0.0635\n",
            "Epoch 4 Batch 400 Loss 0.0733\n",
            "Epoch 4 Batch 450 Loss 0.0719\n",
            "Epoch 4 Batch 500 Loss 0.0859\n",
            "Epoch 4 Batch 550 Loss 0.0692\n",
            "Epoch 4 Batch 600 Loss 0.0714\n",
            "Epoch 4 Loss 0.0741\n",
            "Time taken for 1 epoch 547.102427482605 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0691\n",
            "Epoch 5 Batch 50 Loss 0.0900\n",
            "Epoch 5 Batch 100 Loss 0.0703\n",
            "Epoch 5 Batch 150 Loss 0.0682\n",
            "Epoch 5 Batch 200 Loss 0.0759\n",
            "Epoch 5 Batch 250 Loss 0.0654\n",
            "Epoch 5 Batch 300 Loss 0.0708\n",
            "Epoch 5 Batch 350 Loss 0.0577\n",
            "Epoch 5 Batch 400 Loss 0.0476\n",
            "Epoch 5 Batch 450 Loss 0.0436\n",
            "Epoch 5 Batch 500 Loss 0.0546\n",
            "Epoch 5 Batch 550 Loss 0.0708\n",
            "Epoch 5 Batch 600 Loss 0.0570\n",
            "Epoch 5 Loss 0.0603\n",
            "Time taken for 1 epoch 538.9771716594696 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "# initial_state = encoder.initialize_state()\n",
        "# enc_sequential, enc_final_h, enc_final_c = encoder(source_batch, initial_state)\n",
        "# sample_decoder_output, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final_h, enc_final_c, enc_sequential)\n",
        "# decoder returns: y, dec_h, dec_c, attention_weights\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    #initial_state = encoder.initialize_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "                                                        # inp: (batch_size, Tx)\n",
        "                                                        # targ: (batch_size, Ty)\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_sequential, enc_final_h, enc_final_c = encoder(inp)\n",
        "            \n",
        "            dec_h = enc_final_h\n",
        "            dec_c = enc_final_c\n",
        "            \n",
        "            dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)    # (m, 1)   \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_h, dec_c, _ = decoder(dec_input, dec_h, dec_c, enc_sequential) # predictions = (m, vocab_size)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # update dec_input for teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "    \n",
        "    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMNFHz8eWUxi",
        "outputId": "7e001756-a190-4e20-88b4-db56e3519a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.0451\n",
            "Epoch 1 Batch 50 Loss 0.0565\n",
            "Epoch 1 Batch 100 Loss 0.0497\n",
            "Epoch 1 Batch 150 Loss 0.0493\n",
            "Epoch 1 Batch 200 Loss 0.0503\n",
            "Epoch 1 Batch 250 Loss 0.0515\n",
            "Epoch 1 Batch 300 Loss 0.0537\n",
            "Epoch 1 Batch 350 Loss 0.0453\n",
            "Epoch 1 Batch 400 Loss 0.0473\n",
            "Epoch 1 Batch 450 Loss 0.0275\n",
            "Epoch 1 Batch 500 Loss 0.0484\n",
            "Epoch 1 Batch 550 Loss 0.0397\n",
            "Epoch 1 Batch 600 Loss 0.0483\n",
            "Epoch 1 Loss 0.0485\n",
            "Time taken for 1 epoch 543.0466475486755 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0550\n",
            "Epoch 2 Batch 50 Loss 0.0472\n",
            "Epoch 2 Batch 100 Loss 0.0486\n",
            "Epoch 2 Batch 150 Loss 0.0537\n",
            "Epoch 2 Batch 200 Loss 0.0343\n",
            "Epoch 2 Batch 250 Loss 0.0402\n",
            "Epoch 2 Batch 300 Loss 0.0426\n",
            "Epoch 2 Batch 350 Loss 0.0413\n",
            "Epoch 2 Batch 400 Loss 0.0415\n",
            "Epoch 2 Batch 450 Loss 0.0222\n",
            "Epoch 2 Batch 500 Loss 0.0330\n",
            "Epoch 2 Batch 550 Loss 0.0359\n",
            "Epoch 2 Batch 600 Loss 0.0379\n",
            "Epoch 2 Loss 0.0388\n",
            "Time taken for 1 epoch 545.248651266098 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0337\n",
            "Epoch 3 Batch 50 Loss 0.0362\n",
            "Epoch 3 Batch 100 Loss 0.0304\n",
            "Epoch 3 Batch 150 Loss 0.0385\n",
            "Epoch 3 Batch 200 Loss 0.0315\n",
            "Epoch 3 Batch 250 Loss 0.0341\n",
            "Epoch 3 Batch 300 Loss 0.0344\n",
            "Epoch 3 Batch 350 Loss 0.0226\n",
            "Epoch 3 Batch 400 Loss 0.0261\n",
            "Epoch 3 Batch 450 Loss 0.0179\n",
            "Epoch 3 Batch 500 Loss 0.0319\n",
            "Epoch 3 Batch 550 Loss 0.0275\n",
            "Epoch 3 Batch 600 Loss 0.0251\n",
            "Epoch 3 Loss 0.0309\n",
            "Time taken for 1 epoch 543.7973124980927 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0268\n",
            "Epoch 4 Batch 50 Loss 0.0256\n",
            "Epoch 4 Batch 100 Loss 0.0238\n",
            "Epoch 4 Batch 150 Loss 0.0234\n",
            "Epoch 4 Batch 200 Loss 0.0247\n",
            "Epoch 4 Batch 250 Loss 0.0270\n",
            "Epoch 4 Batch 300 Loss 0.0229\n",
            "Epoch 4 Batch 350 Loss 0.0216\n",
            "Epoch 4 Batch 400 Loss 0.0252\n",
            "Epoch 4 Batch 450 Loss 0.0197\n",
            "Epoch 4 Batch 500 Loss 0.0213\n",
            "Epoch 4 Batch 550 Loss 0.0222\n",
            "Epoch 4 Batch 600 Loss 0.0260\n",
            "Epoch 4 Loss 0.0247\n",
            "Time taken for 1 epoch 540.7813773155212 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0218\n",
            "Epoch 5 Batch 50 Loss 0.0313\n",
            "Epoch 5 Batch 100 Loss 0.0193\n",
            "Epoch 5 Batch 150 Loss 0.0271\n",
            "Epoch 5 Batch 200 Loss 0.0251\n",
            "Epoch 5 Batch 250 Loss 0.0188\n",
            "Epoch 5 Batch 300 Loss 0.0192\n",
            "Epoch 5 Batch 350 Loss 0.0136\n",
            "Epoch 5 Batch 400 Loss 0.0162\n",
            "Epoch 5 Batch 450 Loss 0.0126\n",
            "Epoch 5 Batch 500 Loss 0.0212\n",
            "Epoch 5 Batch 550 Loss 0.0162\n",
            "Epoch 5 Batch 600 Loss 0.0203\n",
            "Epoch 5 Loss 0.0195\n",
            "Time taken for 1 epoch 540.3681843280792 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "# initial_state = encoder.initialize_state()\n",
        "# enc_sequential, enc_final_h, enc_final_c = encoder(source_batch, initial_state)\n",
        "# sample_decoder_output, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final_h, enc_final_c, enc_sequential)\n",
        "# decoder returns: y, dec_h, dec_c, attention_weights\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    #initial_state = encoder.initialize_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "                                                        # inp: (batch_size, Tx)\n",
        "                                                        # targ: (batch_size, Ty)\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_sequential, enc_final_h, enc_final_c = encoder(inp)\n",
        "            \n",
        "            dec_h = enc_final_h\n",
        "            dec_c = enc_final_c\n",
        "            \n",
        "            dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)    # (m, 1)   \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_h, dec_c, _ = decoder(dec_input, dec_h, dec_c, enc_sequential) # predictions = (m, vocab_size)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # update dec_input for teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "    \n",
        "    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgQv_Msa9RL3",
        "outputId": "0d78afcd-ec0b-403c-fad7-9d80feac2a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.0200\n",
            "Epoch 1 Batch 50 Loss 0.0201\n",
            "Epoch 1 Batch 100 Loss 0.0144\n",
            "Epoch 1 Batch 150 Loss 0.0224\n",
            "Epoch 1 Batch 200 Loss 0.0236\n",
            "Epoch 1 Batch 250 Loss 0.0191\n",
            "Epoch 1 Batch 300 Loss 0.0175\n",
            "Epoch 1 Batch 350 Loss 0.0165\n",
            "Epoch 1 Batch 400 Loss 0.0150\n",
            "Epoch 1 Batch 450 Loss 0.0125\n",
            "Epoch 1 Batch 500 Loss 0.0119\n",
            "Epoch 1 Batch 550 Loss 0.0104\n",
            "Epoch 1 Batch 600 Loss 0.0131\n",
            "Epoch 1 Loss 0.0157\n",
            "Time taken for 1 epoch 550.7450428009033 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0146\n",
            "Epoch 2 Batch 50 Loss 0.0137\n",
            "Epoch 2 Batch 100 Loss 0.0143\n",
            "Epoch 2 Batch 150 Loss 0.0131\n",
            "Epoch 2 Batch 200 Loss 0.0126\n",
            "Epoch 2 Batch 250 Loss 0.0161\n",
            "Epoch 2 Batch 300 Loss 0.0126\n",
            "Epoch 2 Batch 350 Loss 0.0100\n",
            "Epoch 2 Batch 400 Loss 0.0118\n",
            "Epoch 2 Batch 450 Loss 0.0099\n",
            "Epoch 2 Batch 500 Loss 0.0152\n",
            "Epoch 2 Batch 550 Loss 0.0157\n",
            "Epoch 2 Batch 600 Loss 0.0104\n",
            "Epoch 2 Loss 0.0134\n",
            "Time taken for 1 epoch 551.1150047779083 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0122\n",
            "Epoch 3 Batch 50 Loss 0.0151\n",
            "Epoch 3 Batch 100 Loss 0.0093\n",
            "Epoch 3 Batch 150 Loss 0.0131\n",
            "Epoch 3 Batch 200 Loss 0.0111\n",
            "Epoch 3 Batch 250 Loss 0.0096\n",
            "Epoch 3 Batch 300 Loss 0.0132\n",
            "Epoch 3 Batch 350 Loss 0.0155\n",
            "Epoch 3 Batch 400 Loss 0.0098\n",
            "Epoch 3 Batch 450 Loss 0.0049\n",
            "Epoch 3 Batch 500 Loss 0.0100\n",
            "Epoch 3 Batch 550 Loss 0.0075\n",
            "Epoch 3 Batch 600 Loss 0.0102\n",
            "Epoch 3 Loss 0.0119\n",
            "Time taken for 1 epoch 547.3289608955383 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0133\n",
            "Epoch 4 Batch 50 Loss 0.0093\n",
            "Epoch 4 Batch 100 Loss 0.0102\n",
            "Epoch 4 Batch 150 Loss 0.0116\n",
            "Epoch 4 Batch 200 Loss 0.0129\n",
            "Epoch 4 Batch 250 Loss 0.0173\n",
            "Epoch 4 Batch 300 Loss 0.0134\n",
            "Epoch 4 Batch 350 Loss 0.0097\n",
            "Epoch 4 Batch 400 Loss 0.0109\n",
            "Epoch 4 Batch 450 Loss 0.0083\n",
            "Epoch 4 Batch 500 Loss 0.0105\n",
            "Epoch 4 Batch 550 Loss 0.0078\n",
            "Epoch 4 Batch 600 Loss 0.0107\n",
            "Epoch 4 Loss 0.0106\n",
            "Time taken for 1 epoch 545.8463060855865 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0117\n",
            "Epoch 5 Batch 50 Loss 0.0090\n",
            "Epoch 5 Batch 100 Loss 0.0068\n",
            "Epoch 5 Batch 150 Loss 0.0067\n",
            "Epoch 5 Batch 200 Loss 0.0124\n",
            "Epoch 5 Batch 250 Loss 0.0133\n",
            "Epoch 5 Batch 300 Loss 0.0123\n",
            "Epoch 5 Batch 350 Loss 0.0085\n",
            "Epoch 5 Batch 400 Loss 0.0087\n",
            "Epoch 5 Batch 450 Loss 0.0053\n",
            "Epoch 5 Batch 500 Loss 0.0074\n",
            "Epoch 5 Batch 550 Loss 0.0053\n",
            "Epoch 5 Batch 600 Loss 0.0088\n",
            "Epoch 5 Loss 0.0094\n",
            "Time taken for 1 epoch 549.2568762302399 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "# initial_state = encoder.initialize_state()\n",
        "# enc_sequential, enc_final_h, enc_final_c = encoder(source_batch, initial_state)\n",
        "# sample_decoder_output, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final_h, enc_final_c, enc_sequential)\n",
        "# decoder returns: y, dec_h, dec_c, attention_weights\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    #initial_state = encoder.initialize_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "                                                        # inp: (batch_size, Tx)\n",
        "                                                        # targ: (batch_size, Ty)\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_sequential, enc_final_h, enc_final_c = encoder(inp)\n",
        "            \n",
        "            dec_h = enc_final_h\n",
        "            dec_c = enc_final_c\n",
        "            \n",
        "            dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)    # (m, 1)   \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_h, dec_c, _ = decoder(dec_input, dec_h, dec_c, enc_sequential) # predictions = (m, vocab_size)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # update dec_input for teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "    \n",
        "    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9DrR6N6biTGY"
      },
      "outputs": [],
      "source": [
        "def evaluate(inputs, encoder, decoder, max_source_length, max_target_length):\n",
        "    # inputs = (1, Tx)\n",
        "\n",
        "    \n",
        "    input_sentence = ''\n",
        "    for i in inputs[0]:\n",
        "        if i == 0:\n",
        "            break\n",
        "        input_sentence = input_sentence + source_sentence_tokenizer.index_word[i] + ' '\n",
        "    #input_sentence = input_sentence[:-1]\n",
        "    \n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    # Encoder: \n",
        "    # Input: x, init_state\n",
        "    # Return: enc_sequential, enc_final_h, enc_final_c\n",
        "\n",
        "    init_state = [tf.zeros((1, units))]\n",
        "    enc_sequential, enc_final_h, enc_final_c = encoder(inputs)\n",
        "\n",
        "    dec_h = enc_final_h\n",
        "    dec_c = enc_final_c\n",
        "    \n",
        "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)             # dec_input = (1, 1)\n",
        "\n",
        "    # Decoder:\n",
        "    # Input: y, dec_h, dec_c, enc_sequential\n",
        "    # Return: y, dec_h, dec_c, attention_weights\n",
        "\n",
        "    # start decoding\n",
        "    for t in range(max_target_length): # limit the length of the decoded sequence\n",
        "        predictions, dec_h, dec_c, attention_weights = decoder(dec_input, dec_h, dec_c, enc_sequential)    # predictions = (1, vocab_size)\n",
        "                                                                                                           # dec_h = (1, 2*units)\n",
        "                                                                                                           # dec_c = (1, 2*units)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        # stop decoding if '_end' is predicted\n",
        "        if target_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
        "            return result, input_sentence\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)                                         # dec_input = (1,1)  \n",
        "\n",
        "    return result, input_sentence\n",
        "  \n",
        "def predict_random_val_sentence():\n",
        "    \n",
        "    k = np.random.randint(len(source_train_tensor))\n",
        "    random_input = source_train_tensor[k]\n",
        "    random_output = target_train_tensor[k]\n",
        "    random_input = np.expand_dims(random_input,0)           # random_input = (1, Tx)\n",
        "    result, sentence = evaluate(random_input, encoder, decoder, max_source_length, max_target_length)\n",
        "    print(f'Input: {sentence[7:-5]}')                   # Want to skip \"start_ \" and \" _end\"\n",
        "    print(f'Predicted translation: {result[:-5]}')\n",
        "    true_translation = ''\n",
        "    for i in random_output:\n",
        "        if i == 0:\n",
        "            break\n",
        "        true_translation = true_translation + target_sentence_tokenizer.index_word[i] + ' '\n",
        "    true_translation = true_translation[7:-6]               # Want to skip \"start_\" and \" _end \"\n",
        "    print(f'Actual translation: {true_translation}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjTwgsVjiTGY",
        "outputId": "2fede915-795e-4e08-be00-076d60ed3f9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: tom bent down and picked up a stone \n",
            "Predicted translation: tom bestellte sich und nahm einen kuchen \n",
            "Actual translation: tom beugte sich herab und nahm einen stein auf\n",
            "None\n",
            "Input: my cat is sleeping on my bed \n",
            "Predicted translation: meine katze schlaeft auf meinem bett \n",
            "Actual translation: meine katze schlaeft auf meinem bett\n",
            "None\n",
            "Input: we have so many students \n",
            "Predicted translation: wir haben so viele schueler \n",
            "Actual translation: wir haben so viele schueler\n",
            "None\n",
            "Input: tom often complains about mosquitoes \n",
            "Predicted translation: tom beschwert sich haeufig ueber muecken \n",
            "Actual translation: tom beschwert sich haeufig ueber muecken\n",
            "None\n",
            "Input: tom listened to the chirping of the birds \n",
            "Predicted translation: tom hoerte wie sich in die operation versteckt \n",
            "Actual translation: tom lauschte dem gezwitscher der voegel\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF3mJQ0siTGZ",
        "outputId": "cf28cb30-d80e-41c8-f037-d5327dcb3dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: tom doesnt know if he can do what youre asking him to do \n",
            "Predicted translation: tom weiss nicht ob er zu dem worum du ihn batst in der lage ist \n",
            "Actual translation: tom weiss nicht ob er zu dem worum du ihn batst in der lage ist\n",
            "None\n",
            "Input: i thought you knew me \n",
            "Predicted translation: ich dachte du wuerdest mir verstanden \n",
            "Actual translation: ich dachte du kenntest mich\n",
            "None\n",
            "Input: it has to mean something \n",
            "Predicted translation: es muss etwas wofuer es bedeutet \n",
            "Actual translation: das muss was bedeuten\n",
            "None\n",
            "Input: tom is safe \n",
            "Predicted translation: tom ist in sicherheit \n",
            "Actual translation: tom ist in sicherheit\n",
            "None\n",
            "Input: were up a creek without a paddle \n",
            "Predicted translation: wir sind in einer schwierigen lage \n",
            "Actual translation: wir sind in einer schwierigen lage\n",
            "None\n",
            "Input: the surgeon who did toms operation is very experienced and highly regarded \n",
            "Predicted translation: der groesste junge hat tom sehr gute spieler der dunkelheit \n",
            "Actual translation: der chirurg der tom operierte ist sehr erfahren und hoch angesehen\n",
            "None\n",
            "Input: he passed away several days before his hundredth birthday \n",
            "Predicted translation: er ist fuenf jahre weit jahre weit von ihm weg \n",
            "Actual translation: er ist ein paar tage vor seinem hundertsten geburtstag gestorben\n",
            "None\n",
            "Input: tom said hed do that without hesitation \n",
            "Predicted translation: tom sagte er wuerde das kaum zu tun \n",
            "Actual translation: tom sagte er wuerde das ohne zu zoegern tun\n",
            "None\n",
            "Input: i saw a moose today \n",
            "Predicted translation: ich habe heute einen elch gesehen \n",
            "Actual translation: ich habe heute einen elch gesehen\n",
            "None\n",
            "Input: here are my papers \n",
            "Predicted translation: hier sind meine papiere \n",
            "Actual translation: hier sind meine papiere\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un6VwcDqiTGZ",
        "outputId": "7958a66d-428a-4557-e022-6d4ee1d34f4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: i work very hard \n",
            "Predicted translation: ich esse sehr gut \n",
            "Actual translation: ich arbeite sehr hart\n",
            "None\n",
            "Input: could you please do me a favor \n",
            "Predicted translation: koenntest du mir bitte einen gefallen tun \n",
            "Actual translation: koennten sie mir bitte einen gefallen tun\n",
            "None\n",
            "Input: i wish id been born a canadian \n",
            "Predicted translation: ich wuenschte ich waere als kanadierin geboren \n",
            "Actual translation: ich waere gerne als kanadier geboren worden\n",
            "None\n",
            "Input: have you found your glasses yet not yet \n",
            "Predicted translation: haben sie schon mal eine chance schon gestern gemacht \n",
            "Actual translation: hast du deine brille schon gefunden noch nicht\n",
            "None\n",
            "Input: i brought tom some fruit \n",
            "Predicted translation: ich habe tom etwas obst mitgebracht \n",
            "Actual translation: ich habe tom etwas obst mitgebracht\n",
            "None\n",
            "Input: can you pay attention please \n",
            "Predicted translation: kannst du bitte aufpassen \n",
            "Actual translation: koenntest du bitte aufpassen\n",
            "None\n",
            "Input: he is about thirty \n",
            "Predicted translation: er ist etwa dreissig \n",
            "Actual translation: er ist ungefaehr dreissig\n",
            "None\n",
            "Input: lets see you do that \n",
            "Predicted translation: sehen wir du dabei \n",
            "Actual translation: lass sehen dass du das tust\n",
            "None\n",
            "Input: when did you get the concert ticket \n",
            "Predicted translation: wann hast du die konzertkarte bekommen \n",
            "Actual translation: wann hast du die konzertkarte bekommen\n",
            "None\n",
            "Input: this door is locked from the inside \n",
            "Predicted translation: diese tuer ist von einem laster vertraut \n",
            "Actual translation: diese tuer ist von innen verschlossen\n",
            "None\n",
            "Input: tom froze \n",
            "Predicted translation: tom motzte \n",
            "Actual translation: tom erstarrte\n",
            "None\n",
            "Input: there is a big stack of mail on the table \n",
            "Predicted translation: auf dem tisch liegt ein grosser stapel post \n",
            "Actual translation: auf dem tisch liegt ein grosser stapel post\n",
            "None\n",
            "Input: i guess you dont really care \n",
            "Predicted translation: ich schaetze es interessiert dich nicht wirklich \n",
            "Actual translation: ich schaetze es interessiert dich nicht wirklich\n",
            "None\n",
            "Input: theres a reservoir near where i live \n",
            "Predicted translation: in meiner nachbarschaft gibt es in der naehe meines parks \n",
            "Actual translation: in der naehe meines wohnortes gibt es einen stausee\n",
            "None\n",
            "Input: we hope you all have a lovely evening \n",
            "Predicted translation: wir kennen alle alle einen schoenen abend \n",
            "Actual translation: wir wuenschen euch allen einen schoenen abend\n",
            "None\n",
            "Input: she tried in vain not to cry \n",
            "Predicted translation: sie versuchte vergeblich nicht zu weinen \n",
            "Actual translation: sie versuchte vergeblich nicht zu weinen\n",
            "None\n",
            "Input: graham greene is my favorite author \n",
            "Predicted translation: graham greene ist mein lieblingsschriftsteller \n",
            "Actual translation: graham greene ist mein lieblingsschriftsteller\n",
            "None\n",
            "Input: you have no idea do you \n",
            "Predicted translation: sie haben keine ahnung oder \n",
            "Actual translation: du hast keine ahnung oder\n",
            "None\n",
            "Input: ill see you later tonight \n",
            "Predicted translation: ich werde ihnen heute abend sehen \n",
            "Actual translation: wir sehen uns heute abend\n",
            "None\n",
            "Input: tom forgot the ketchup \n",
            "Predicted translation: tom hat das ketchup vergessen \n",
            "Actual translation: tom hat das ketchup vergessen\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())\n",
        "print (predict_random_val_sentence())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tlr4zk_iTGZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvAcO44PiTGZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3H8exxwiTGZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAI-6i9IiTGZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GUC5EJsiTGZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "attention_class_part_3.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('deep_learning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
