{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data source: http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This file also works. Now using pre-trained embeddings.\n",
    "- Used functions for code blocks. But that didn't really make things much neater. \n",
    "- Bidirectional LSTM for encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, RNN, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "import regex as re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "import spacy\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "# disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: x.lower())\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "# Convert umlauts and sharp s:\n",
    "df_en_de['german'] = df_en_de['german'].apply(\n",
    "                            lambda x: x.replace('ü', 'ue').replace('ä', 'ae').replace('ö', 'oe').replace('ß', 'ss')\n",
    "                            )\n",
    "\n",
    "# Set of all special characters\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# Remove all the special characters\n",
    "df_en_de['english'] = df_en_de['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "df_en_de['german']=df_en_de['german'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "df_en_de['german'] = df_en_de['german'].apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename dataframe for convenience\n",
    "pairs = df_en_de\n",
    "max_len = 10\n",
    "\n",
    "pairs = df_en_de\n",
    "pairs['english_length'] = pairs['english'].apply(lambda x: len(x.split(' ')))\n",
    "pairs['german_length'] = pairs['german'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "# remove ascii symbols\n",
    "pairs['english_cleaned'] = pairs['english'].apply(lambda x: x.encode(\"ascii\", \"ignore\"))\n",
    "pairs['english_cleaned'] = pairs['english_cleaned'].apply(lambda x: x.decode())\n",
    "\n",
    "# remove ascii symbols\n",
    "pairs['german_cleaned'] = pairs['german'].apply(lambda x: x.encode(\"ascii\", \"ignore\"))\n",
    "pairs['german_cleaned'] = pairs['german_cleaned'].apply(lambda x: x.decode())\n",
    "\n",
    "pairs = pairs[pairs['english_length'] <= max_len]\n",
    "pairs = pairs[pairs['german_length'] <= max_len]\n",
    "\n",
    "# Reduced dataset to check if model works\n",
    "pairs = pairs.sample(frac = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg\n",
    "import en_core_web_lg\n",
    "\n",
    "!python -m spacy download de_core_news_sm\n",
    "import de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_source = pairs['english_cleaned']\n",
    "text_target = pairs['german_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_source = en_core_web_lg.load()\n",
    "nlp_target = de_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 16:48:10.835211: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "Vectorizer_source = TextVectorization()\n",
    "Vectorizer_target = TextVectorization()\n",
    "\n",
    "Vectorizer_source.adapt(text_source)\n",
    "Vectorizer_target.adapt(text_target)\n",
    "\n",
    "vocab_source = Vectorizer_source.get_vocabulary()\n",
    "vocab_target = Vectorizer_target.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of words\n",
    "vocab_source = [str(word) for word in vocab_source]\n",
    "vocab_target = [str(word) for word in vocab_target]\n",
    "\n",
    "# remove empty string from vocab\n",
    "vocab_source.remove('')\n",
    "vocab_target.remove('')\n",
    "\n",
    "# The embeddings downloaded from spacy don't include our 'START_' and '_END' tokens\n",
    "# Add them to \"vocab_target\"\n",
    "vocab_target.append('START_')\n",
    "vocab_target.append('_END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6446, 9992)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size for source and target\n",
    "\n",
    "vocab_len_source = len(vocab_source)\n",
    "vocab_len_target = len (vocab_target)\n",
    "\n",
    "vocab_len_source, vocab_len_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the embedding matrix for source vocab\n",
    "\n",
    "# add 1 for zero padding (for Embedding layer)\n",
    "num_tokens_source = len(vocab_source)\n",
    "num_tokens_source = num_tokens_source + 1\n",
    "\n",
    "# source language embedding dimensions\n",
    "embedding_dim_source = len(nlp_source('The').vector)\n",
    "# initialise embedding matrix for source language\n",
    "embedding_matrix_source = np.zeros((num_tokens_source, embedding_dim_source))\n",
    "\n",
    "# word-to-index and index-to-word mappings for source language\n",
    "word_idx_source = {}\n",
    "idx_word_source = {}\n",
    "# notice we start indexing from 1 (no word is assigned to 0 index)\n",
    "for i, word in enumerate(vocab_source):\n",
    "    embedding_matrix_source[i+1] = nlp_source(word).vector      # load vectors into embedding matrix\n",
    "    word_idx_source[word] = int(i+1)                            # word-to-index map\n",
    "    idx_word_source[i+1] = word                                 # index-to-word map\n",
    "\n",
    "# generate the embedding matrix for target vocab\n",
    "\n",
    "# add 1 for zero padding (for Embedding layer)\n",
    "num_tokens_target = len(vocab_target)\n",
    "num_tokens_target = num_tokens_target + 1\n",
    "\n",
    "# target language embedding dimensions\n",
    "embedding_dim_target = len(nlp_target('Der').vector)\n",
    "# initialise embedding matrix for target language\n",
    "embedding_matrix_target = np.zeros((num_tokens_target, embedding_dim_target))\n",
    "\n",
    "# word-to-index and index-to-word mappings for target language\n",
    "word_idx_target = {}\n",
    "idx_word_target = {}\n",
    "for i, word in enumerate(vocab_target):\n",
    "    # iterate over all words excluding the final two (\"START_\" and \"_END\")\n",
    "    if i < len(vocab_target)-2: \n",
    "        embedding_matrix_target[i+1] = nlp_target(word).vector      # load vectors into embedding matrix\n",
    "        word_idx_target[word] = int(i+1)                            # word-to-index map\n",
    "        idx_word_target[i+1] = word                                 # index-to-word map\n",
    "    if word == 'START_':\n",
    "        # assign embedding vector with random values for \"START_\" token \n",
    "        embedding_matrix_target[i+1] = np.random.rand((embedding_dim_target))\n",
    "        word_idx_target[word] = int(i+1)\n",
    "        idx_word_target[i+1] = word\n",
    "    if word == '_END':\n",
    "        # assign embedding vector with random values for \"_END\" token\n",
    "        embedding_matrix_target[i+1] = np.random.rand((embedding_dim_target))\n",
    "        word_idx_target[word] = int(i+1)\n",
    "        idx_word_target[i+1] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run time for small dataset: 1 m 4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9993, 9992)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_source, len(vocab_source)\n",
    "num_tokens_target, len(vocab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to load CLEANED data\n",
    "X, y = pairs['english_cleaned'], pairs['german_cleaned']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        # for every batch j\n",
    "        for j in range(0, len(X), batch_size):          \n",
    "            encoder_input_data = np.zeros((batch_size, max_len), dtype='float32')         # (batch_size, max_len)      \n",
    "            decoder_input_data = np.zeros((batch_size, max_len), dtype='float32')         # (batch_size, max_len)\n",
    "            decoder_target_data = np.zeros((batch_size, max_len, num_tokens_target), dtype='float32') # (batch_size, max_len, num_tokens_target)\n",
    "            \n",
    "            # for every example i\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    # for every time-step t, insert index for encoder input\n",
    "                    encoder_input_data[i, t] = word_idx_source[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    # for every time-step t, insert index for decoder input (excluding final time-step)\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = word_idx_target[word] # decoder input seq\n",
    "                    # create one-hot vector for decoder output, excluding the START_ token\n",
    "                    # offset by one timestep\n",
    "                    if t>0:\n",
    "                        decoder_target_data[i, t - 1, word_idx_target[word]] = 1. \n",
    "                        \n",
    "            \n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- global variables for one_step_attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = max_len\n",
    "Ty = max_len\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis = -1)\n",
    "densor1 = Dense(10, activation = 'tanh')\n",
    "densor2 = Dense (1, activation = 'relu')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention (h, s_prev):\n",
    "\n",
    "    # calculate the Context vector for one time-step of decoder\n",
    "\n",
    "    # h = (m, Tx, n_h)\n",
    "    # s_prev = (m, n_s)\n",
    "    # returns: context – we will then use [context; y_prev] as input of Decoder\n",
    "\n",
    "    s_prev = repeator(s_prev)                   # (m, Tx, n_s)\n",
    "    concat = concatenator([h, s_prev])          # (m, Tx, n_h + n_s)\n",
    "    e = densor1 (concat)                        # (m, Tx, 10)\n",
    "    energies = densor2 (e)                      # (m, Tx, 1)\n",
    "    alphas = tf.nn.softmax(energies, axis = 1)  # (m, Tx, 1)\n",
    "    context = dotor([alphas, h])                # alphas = (m, Tx, 1)\n",
    "                                                # h = (m, Tx, n_h)\n",
    "                                                # (m, 1, n_h)\n",
    "    return context                              # (m, 1, n_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim_source = embedding_matrix_source.shape[1]\n",
    "emb_dim_target = embedding_matrix_target.shape[1]\n",
    "\n",
    "n_h = 200\n",
    "n_s = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- global variables for pre-attention LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = Input(shape = (Tx,))                         # (None, Tx) = (m, Tx)\n",
    "x_emb_layer = Embedding(num_tokens_source, emb_dim_source, mask_zero = True,\n",
    "                        embeddings_initializer = Constant(embedding_matrix_source),\n",
    "                        trainable = False)  \n",
    "\n",
    "x_emb = x_emb_layer(x_inputs)                         # (None, Tx, x_emb_dim) = (m, Tx, x_emb_dim)\n",
    "\n",
    "# Use bidirectional LSTM\n",
    "enc_lstm_layer = Bidirectional(LSTM(n_h, dropout = 0.3, recurrent_dropout = 0.3, return_sequences=True, return_state = True))\n",
    "h_enc, s_enc, c_enc, _, _ = enc_lstm_layer(x_emb)                             # h_enc = (None, Tx, 2 * n_h) = (m, Tx, 2*n_h)\n",
    "                                                                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- global variables for post-attention LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_inputs = Input(shape = (Ty,))                         # (None, Ty) = (m, Ty)\n",
    "\n",
    "y_emb_layer = Embedding (num_tokens_target, \n",
    "                        emb_dim_target, \n",
    "                        mask_zero = True,\n",
    "                        embeddings_initializer = Constant(embedding_matrix_target),\n",
    "                        trainable = False)\n",
    "                        \n",
    "y_emb = y_emb_layer(y_inputs)                         # (None, Ty, y_emb_dim) = (m, Ty, y_emb_dim)\n",
    "\n",
    "dec_lstm_layer = LSTM(n_s, dropout = 0.3, recurrent_dropout = 0.3, return_state = True)\n",
    "output_layer = Dense(num_tokens_target, activation='softmax')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model():\n",
    "\n",
    "    # s<0> and c<0> for decoder = s<Ty> and c<Ty> for encoder\n",
    "                                                                 \n",
    "    s_dec = s_enc\n",
    "    c_dec = c_enc\n",
    "    \n",
    "    outputs = []\n",
    "    for t in range(Ty):\n",
    "        context = one_step_attention(h_enc, s_dec)                                            # context = (m, 1, n_h)\n",
    "        concat = Concatenate(axis = -1)([context, tf.expand_dims(y_emb[:,t,:],1)])            # concat = (m, 1, n_h + y_emb_dim)\n",
    "        \n",
    "        # update decoder LSTM hidden state (s) and cell state (c)\n",
    "        _, s_dec, c_dec = dec_lstm_layer (initial_state = [s_dec, c_dec], inputs = concat)        # s = (None, Ty, n_s)\n",
    "        \n",
    "        # pass decoder LSTM hidden state (s) through output layer to get y prediction\n",
    "        out = output_layer(s_dec)                                                                   # out = (m, num_decoder_tokens)\n",
    "        outputs.append(out)\n",
    "                                                                \n",
    "    outputs = tf.stack(outputs, axis = 1)                                                           # outputs = (m, Ty, num_decoder_tokens)\n",
    "    training_model = Model(inputs = [x_inputs, y_inputs], outputs = outputs)\n",
    "\n",
    "    return training_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = training_model()\n",
    "training_model.compile(optimizer= 'Adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "train_samples = len(X_train) # Total Training samples\n",
    "val_samples = len(X_test) # total validation samples\n",
    "batch_size = 128\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "130/130 [==============================] - 66s 340ms/step - loss: 3.9550 - acc: 0.1273 - val_loss: 3.6071 - val_acc: 0.1514\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 41s 316ms/step - loss: 3.3487 - acc: 0.1695 - val_loss: 3.2495 - val_acc: 0.1898\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 40s 306ms/step - loss: 3.0277 - acc: 0.2037 - val_loss: 3.0147 - val_acc: 0.2142\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 40s 312ms/step - loss: 2.7801 - acc: 0.2230 - val_loss: 2.8306 - val_acc: 0.2323\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 42s 319ms/step - loss: 2.5814 - acc: 0.2402 - val_loss: 2.6914 - val_acc: 0.2467\n"
     ]
    }
   ],
   "source": [
    "history = training_model.fit(generate_batch(), \n",
    "                    steps_per_epoch=train_samples//batch_size, \n",
    "                    epochs = 5,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples // batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.514828</td>\n",
       "      <td>0.332356</td>\n",
       "      <td>2.134959</td>\n",
       "      <td>0.311377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.454318</td>\n",
       "      <td>0.339177</td>\n",
       "      <td>2.110950</td>\n",
       "      <td>0.315015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.399507</td>\n",
       "      <td>0.346094</td>\n",
       "      <td>2.092885</td>\n",
       "      <td>0.317871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.346242</td>\n",
       "      <td>0.355060</td>\n",
       "      <td>2.074517</td>\n",
       "      <td>0.320654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.300279</td>\n",
       "      <td>0.361382</td>\n",
       "      <td>2.054834</td>\n",
       "      <td>0.322803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.251908</td>\n",
       "      <td>0.370853</td>\n",
       "      <td>2.045037</td>\n",
       "      <td>0.325537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.209428</td>\n",
       "      <td>0.379495</td>\n",
       "      <td>2.033970</td>\n",
       "      <td>0.327197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.169399</td>\n",
       "      <td>0.386148</td>\n",
       "      <td>2.017985</td>\n",
       "      <td>0.330762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.132447</td>\n",
       "      <td>0.394736</td>\n",
       "      <td>2.005462</td>\n",
       "      <td>0.330737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.095821</td>\n",
       "      <td>0.402019</td>\n",
       "      <td>1.992962</td>\n",
       "      <td>0.333887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  val_loss   val_acc\n",
       "0  1.514828  0.332356  2.134959  0.311377\n",
       "1  1.454318  0.339177  2.110950  0.315015\n",
       "2  1.399507  0.346094  2.092885  0.317871\n",
       "3  1.346242  0.355060  2.074517  0.320654\n",
       "4  1.300279  0.361382  2.054834  0.322803\n",
       "5  1.251908  0.370853  2.045037  0.325537\n",
       "6  1.209428  0.379495  2.033970  0.327197\n",
       "7  1.169399  0.386148  2.017985  0.330762\n",
       "8  1.132447  0.394736  2.005462  0.330737\n",
       "9  1.095821  0.402019  1.992962  0.333887"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_csv_file = 'history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.514828</td>\n",
       "      <td>0.332356</td>\n",
       "      <td>2.134959</td>\n",
       "      <td>0.311377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.454318</td>\n",
       "      <td>0.339177</td>\n",
       "      <td>2.110950</td>\n",
       "      <td>0.315015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.399507</td>\n",
       "      <td>0.346094</td>\n",
       "      <td>2.092885</td>\n",
       "      <td>0.317871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.346242</td>\n",
       "      <td>0.355060</td>\n",
       "      <td>2.074517</td>\n",
       "      <td>0.320654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.300279</td>\n",
       "      <td>0.361382</td>\n",
       "      <td>2.054834</td>\n",
       "      <td>0.322803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.251908</td>\n",
       "      <td>0.370853</td>\n",
       "      <td>2.045037</td>\n",
       "      <td>0.325537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.209428</td>\n",
       "      <td>0.379495</td>\n",
       "      <td>2.033970</td>\n",
       "      <td>0.327197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.169399</td>\n",
       "      <td>0.386148</td>\n",
       "      <td>2.017985</td>\n",
       "      <td>0.330762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.132447</td>\n",
       "      <td>0.394736</td>\n",
       "      <td>2.005462</td>\n",
       "      <td>0.330737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.095821</td>\n",
       "      <td>0.402019</td>\n",
       "      <td>1.992962</td>\n",
       "      <td>0.333887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  val_loss   val_acc\n",
       "0  1.514828  0.332356  2.134959  0.311377\n",
       "1  1.454318  0.339177  2.110950  0.315015\n",
       "2  1.399507  0.346094  2.092885  0.317871\n",
       "3  1.346242  0.355060  2.074517  0.320654\n",
       "4  1.300279  0.361382  2.054834  0.322803\n",
       "5  1.251908  0.370853  2.045037  0.325537\n",
       "6  1.209428  0.379495  2.033970  0.327197\n",
       "7  1.169399  0.386148  2.017985  0.330762\n",
       "8  1.132447  0.394736  2.005462  0.330737\n",
       "9  1.095821  0.402019  1.992962  0.333887"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_model():\n",
    "    # x_inputs defined above as Input\n",
    "    # h_enc, s_enc, c_enc also defined above\n",
    "    encoder_model = Model(inputs = x_inputs, outputs = [h_enc, s_enc, c_enc])\n",
    "    return encoder_model\n",
    "\n",
    "def decoder_model():\n",
    "    # The below three decoder inputs will come from encoder_model.predict()\n",
    "    decoder_input_h = Input(shape = (Tx, 2*n_h))               # (None, Tx, 2*n_h) \n",
    "    decoder_input_s = Input(shape=(n_h, ))                    # (None, n_h) \n",
    "    decoder_input_c = Input(shape = (n_h,))                   # (None, n_h)\n",
    "\n",
    "    # y_emb_2 will be our y_pred at t-1\n",
    "    y_inp_2 = Input(shape = (None,))                        # (None, None) = (m, Ty)\n",
    "    y_emb_2 = y_emb_layer(y_inp_2)                          # (None, None, y_emb_dim) = (m, Ty, y_emb_dim)\n",
    "\n",
    "    # Use decoder_input_s and decoder_input_h to compute context vector\n",
    "    context = one_step_attention(decoder_input_h, decoder_input_s)    # (m, 1, 2*n_h)\n",
    "\n",
    "    # concatenate context with y_emb_2\n",
    "    concat2 = Concatenate(axis = -1)([context, tf.expand_dims(y_emb_2[:,-1,:],1)])                       \n",
    "                                                            # concat2 = (None, 1, 2*n_h + y_emb_dim)\n",
    "\n",
    "    # Feed concat2 as input; decoder_input_s and decoder_input_c as initial state\n",
    "    _, decoder_output_s, decoder_output_c = dec_lstm_layer (\n",
    "                                                            initial_state = [decoder_input_s, decoder_input_c], \n",
    "                                                            inputs = concat2\n",
    "                                                            )     \n",
    "                                                # decoder_output_s = (None, n_s) \n",
    "                                                # decoder_output_c = (None, n_s) \n",
    "\n",
    "\n",
    "    decoder_output_y = output_layer(tf.expand_dims(decoder_output_s,1))           # (None, 1, num_decoder_tokens)\n",
    "\n",
    "    decoder_model = Model(inputs = [decoder_input_h, decoder_input_s, decoder_input_c, y_inp_2],\n",
    "                            outputs = [decoder_output_y, decoder_output_s, decoder_output_c])\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = encoder_model()\n",
    "decoder_model = decoder_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_sequence):\n",
    "\n",
    "    # input_sequence = (1, max_len)\n",
    "    \n",
    "    # get hidden states + final hidden state + final cell state from encoder \n",
    "    h_enc_pred, s_enc_pred, c_enc_pred = encoder_model.predict(input_sequence)\n",
    "    \n",
    "    # define y_pred at time 0    \n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0,0] = word_idx_target['START_']\n",
    "\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # initialise hidden state and cell state input for decoder\n",
    "    decoder_s_pred = s_enc_pred                         # (None, n_h) = (m, n_h)\n",
    "    decoder_c_pred = c_enc_pred                         # (None, n_h) = (m, n_h)\n",
    "    \n",
    "    while not stop_condition:\n",
    "        decoder_y_pred, decoder_s_pred, decoder_c_pred = decoder_model.predict([h_enc_pred, decoder_s_pred, decoder_c_pred, target_seq])  \n",
    "        y_index = np.argmax(decoder_y_pred[0,-1,:])\n",
    "        y_word = idx_word_target[y_index]\n",
    "        decoded_sentence += ' ' + y_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (y_word == '_END' or\n",
    "           len(decoded_sentence.split()) > max_len):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = y_index\n",
    "        \n",
    "        \n",
    "        \n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence):\n",
    "    \"\"\"\n",
    "    sentence = string\n",
    "    \"\"\"\n",
    "    \n",
    "    encoder_input_data = np.zeros((1, max_len))     # (1, max_len) \n",
    "    sentence = sentence.lower().split()\n",
    "    for j, word in enumerate(sentence):\n",
    "        encoder_input_data[0,j] = word_idx_source[word]        # (1, max_len)\n",
    "    return encoder_input_data                           # (1, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('why didnt tom visit boston', ' warum tom nicht tom und maria _END')\n",
      "('please dont waste electricity', ' bitte bitte nicht _END')\n",
      "('tom insulted the waiter', ' tom hat die tuer _END')\n",
      "('theres nothing to worry about', ' das ist nicht zu sein _END')\n",
      "('they appointed him manager', ' sie hat ihn zu hause _END')\n",
      "('one more bottle of wine please', ' das buch fuer eine zeit _END')\n",
      "('do you know what tom said about it', ' warum du tom dass tom hat tom _END')\n",
      "('tom is always making me angry', ' tom ist mir sehr nie _END')\n",
      "('i want to speak with tom please', ' ich moechte tom zu helfen _END')\n",
      "('arent you pushing it too far', ' du nicht so so nicht _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('why didnt tom visit boston', ' warum tom nicht nach boston _END')\n",
      "('please dont waste electricity', ' bitte bitte nicht die _END')\n",
      "('tom insulted the waiter', ' tom hat die haare gesehen _END')\n",
      "('theres nothing to worry about', ' es ist nichts zu essen zu tun _END')\n",
      "('they appointed him manager', ' sie haben ihn zu heiraten _END')\n",
      "('one more bottle of wine please', ' ein paar kaffee bitte _END')\n",
      "('do you know what tom said about it', ' weisst du dass tom was tom gesagt _END')\n",
      "('tom is always making me angry', ' tom ist immer immer mich _END')\n",
      "('i want to speak with tom please', ' ich will tom zu helfen _END')\n",
      "('arent you pushing it too far', ' bist du nicht so schnell _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('why didnt tom visit boston', ' warum tom nicht nach boston boston _END')\n",
      "('please dont waste electricity', ' bitte haben keine sorgen _END')\n",
      "('tom insulted the waiter', ' tom hat die haare _END')\n",
      "('theres nothing to worry about', ' es gibt es nichts zu reden _END')\n",
      "('they appointed him manager', ' sie haben ihn angelogen _END')\n",
      "('one more bottle of wine please', ' eine tasse bitte wein _END')\n",
      "('do you know what tom said about it', ' weisst du was tom gesagt hat _END')\n",
      "('tom is always making me angry', ' tom ist immer immer mich wuetend _END')\n",
      "('i want to speak with tom please', ' ich will mit tom helfen _END')\n",
      "('arent you pushing it too far', ' bist du es nicht so schnell _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('why didnt tom visit boston', ' warum ist tom boston nicht nach boston _END')\n",
      "('please dont waste electricity', ' bitte mache keine unnoetigen _END')\n",
      "('tom insulted the waiter', ' tom hat den kellner gemacht _END')\n",
      "('theres nothing to worry about', ' es gibt nichts zu loesen _END')\n",
      "('they appointed him manager', ' sie ernannten ihn angelogen _END')\n",
      "('one more bottle of wine please', ' eine tasse kaffee bitte wein _END')\n",
      "('do you know what tom said about it', ' weisst du was tom darueber erzaehlt _END')\n",
      "('tom is always making me angry', ' tom macht immer immer wuetend _END')\n",
      "('i want to speak with tom please', ' ich will mit tom helfen _END')\n",
      "('arent you pushing it too far', ' bist du nicht so schnell _END')\n"
     ]
    }
   ],
   "source": [
    "sentences = list(X_train.iloc[:10].values)          # list of sentences\n",
    "\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    seq = sentence_to_seq(sentence)             # seq = (1, max_len)\n",
    "    #print(seq, seq.shape)\n",
    "    translation = decode_sequence(seq)\n",
    "    translations.append(translation)\n",
    "\n",
    "sentence_translation_pairs = zip (sentences, translations)\n",
    "for elem in sentence_translation_pairs:\n",
    "    print (elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
