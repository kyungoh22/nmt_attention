{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- functioning NMT model\n",
    "- attention model using classes\n",
    "- uni-directional gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "#import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])\n",
    "df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"'\", '', sentence)\n",
    "    sentence = sentence.replace('ü', 'ue').replace('ä', 'ae').replace('ö', 'oe').replace('ß', 'ss')\n",
    "    exclude = set(string.punctuation)\n",
    "    sentence = ''.join(ch for ch in sentence if ch not in exclude)\n",
    "    sentence = 'start_ ' + sentence + ' _end'\n",
    "    sentence = sentence.encode(\"ascii\", \"ignore\")\n",
    "    sentence = sentence.decode()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = df_en_de\n",
    "pairs = pairs.sample(frac = 0.1)\n",
    "pairs['english'] = pairs['english'].apply(preprocess_sentence)\n",
    "pairs['german'] = pairs['german'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = pairs['english']\n",
    "target = pairs ['german']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer & tensor for source and target\n",
    "source_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "source_sentence_tokenizer.fit_on_texts(source)\n",
    "source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n",
    "source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor, padding='post' )\n",
    "\n",
    "target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "target_sentence_tokenizer.fit_on_texts(target)\n",
    "target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n",
    "target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor = train_test_split(\n",
    "                                                                source_tensor, target_tensor,test_size=0.2\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_length= max(len(t) for t in  target_tensor)\n",
    "max_source_length= max(len(t) for t in source_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 33)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_length, max_source_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "#Create data in memeory \n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BATCH_SIZE)\n",
    "# shuffles the data in the batch\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33) (64, 34)\n",
      "tf.Tensor(\n",
      "[   1   93  194  286    4   60   29 4839 3489    2    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0], shape=(33,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "source_batch, target_batch =next(iter(dataset))\n",
    "print(source_batch.shape, target_batch.shape)\n",
    "print(source_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(source_train_tensor)\n",
    "steps_per_epoch= BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim=256\n",
    "units=1024\n",
    "attention_layer_units = 100\n",
    "source_vocab_size= len(source_sentence_tokenizer.word_index)+1\n",
    "target_vocab_size= len(target_sentence_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding =tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru= tf.keras.layers.GRU(encoder_units, \n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,                                      \n",
    "                                      recurrent_initializer='glorot_uniform'\n",
    "                                     )\n",
    "    def call(self, x, hidden):\n",
    "                                                                # x = (m, Tx)\n",
    "                                                                # hidden = (m, encoder_units)\n",
    "        x = self.embedding(x)                                   # (m, Tx, embedding_dim)\n",
    "                                                                \n",
    "        enc_sequential, enc_final = self.gru(x, initial_state = hidden)\n",
    "        return enc_sequential, enc_final                                    # enc_sequential = (m, Tx, encoder_units) \n",
    "                                                                            # enc_final = (m, encoder_units)\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder sequential: (64, 33, 1024)\n",
      "Encoder final: (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(source_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "enc_final = encoder.initialize_hidden_state()\n",
    "enc_sequential, enc_final = encoder(source_batch, enc_final)\n",
    "\n",
    "print (f'Encoder sequential: {enc_sequential.shape}')\n",
    "print (f'Encoder final: {enc_final.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super( BahdanauAttention, self).__init__()\n",
    "        self.W1= tf.keras.layers.Dense(units)  # decoder hidden\n",
    "        self.W2= tf.keras.layers.Dense(units)  # encoder hidden\n",
    "        self.V= tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, dec_hidden, enc_hidden):\n",
    "                                                                # dec_hidden = (m, units)\n",
    "                                                                # enc_hidden:   (m, Tx, units)\n",
    "        dec_hidden_with_time = tf.expand_dims(dec_hidden, 1)    # dec_hidden_with_time = (m, 1, units)\n",
    "        \n",
    "                                                                \n",
    "        # W1() = (m, 1, 10) \n",
    "        # W2() = (m, Tx, 10)\n",
    "        # Broadcasting happens when you add\n",
    "        # W1() + W2 () = (m, Tx, 10)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(dec_hidden_with_time) + self.W2(enc_hidden))) # (m, Tx, 1)\n",
    "        \n",
    "        # normalise scores with softmax\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)                                # (m, Tx, 1)\n",
    "        \n",
    "        # apply each weight to encoder hidden state at respective time-step \n",
    "        context_vector= attention_weights * enc_hidden                                  # (m, Tx, units)\n",
    "       \n",
    "        # linear combination of enc_hidden vectors for all Tx\n",
    "        # so sum along Tx axis\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)                          # (m, units)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tens1 = tf.Variable([[[1,2],[1,2]], [[1,2],[1,2]], [[1,2],[1,2]]])\n",
    "# tens2 = tf.Variable([[[2],[2]], [[2],[2]], [[2],[2]]])\n",
    "# tens1 * tens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector: (batch size, units) (64, 1024)\n",
      "attention weights: (batch_size, sequence_length, 1) (64, 33, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer= BahdanauAttention(attention_layer_units)\n",
    "attention_result, attention_weights = attention_layer(enc_final, enc_sequential)\n",
    "print(f\"context vector: (batch size, units) {attention_result.shape}\")\n",
    "print(f\"attention weights: (batch_size, sequence_length, 1) {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder for one time-step\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_sz):\n",
    "        super (Decoder,self).__init__()\n",
    "        self.batch_sz= batch_sz\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, \n",
    "                                                   embedding_dim)\n",
    "        self.gru= tf.keras.layers.GRU(decoder_units, \n",
    "                                      return_sequences= True,\n",
    "                                      return_state=True,\n",
    "                          recurrent_initializer='glorot_uniform')\n",
    "        # Fully connected layer\n",
    "        self.fc= tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # attention\n",
    "        #self.attention = BahdanauAttention(self.decoder_units)\n",
    "        self.attention = BahdanauAttention(attention_layer_units)\n",
    "    \n",
    "    def call(self, y, dec_hidden, enc_hidden):\n",
    "                                                                                    # dec_hidden: (m, units) \n",
    "                                                                                    # enc_hidden: (m, Tx, units) \n",
    "\n",
    "        context_vector, attention_weights = self.attention(dec_hidden, enc_hidden)  # context_vector = (m, units)\n",
    "        \n",
    "        y= self.embedding(y)                                                        # y = (m, 1, embedding_dim)\n",
    "        \n",
    "        # concatenate context vector and embedding for output sequence\n",
    "        y = tf.concat([tf.expand_dims(context_vector, 1), y],                       # (m, 1, units) + (m, 1, embedding_dim)\n",
    "                                      axis=-1)                                      # (m, 1, units + embedding_dim)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(y)                                                 # output = (m, 1, units)\n",
    "                                                                                    # state = (m, units)\n",
    "\n",
    "        output= tf.reshape(output, (-1, output.shape[2]))                           # output = (m, units)\n",
    "        \n",
    "        # pass the output thru Fc layers\n",
    "        y = self.fc(output)                                                         # y = (m, vocab_size)\n",
    "        return y, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 12072)\n"
     ]
    }
   ],
   "source": [
    "decoder= Decoder(target_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _= decoder(tf.random.uniform((BATCH_SIZE,1)), enc_final, enc_sequential )\n",
    "print (f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer and the loss function\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):      # \"real\" = (m, 1), \"pred\" = (m, vocab_size)\n",
    "    mask = 1 - np.equal(real, 0)    # mask = 1 when \"real\" != 0\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.9135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_1.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_1.ipynb#ch0000035?line=32'>33</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_1.ipynb#ch0000035?line=34'>35</a>\u001b[0m variables \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mvariables \u001b[39m+\u001b[39m decoder\u001b[39m.\u001b[39mvariables\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_1.ipynb#ch0000035?line=36'>37</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, variables)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_1.ipynb#ch0000035?line=38'>39</a>\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, variables))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Oh/Desktop/Machine_Learning/deep_learning/pet_projects/Attention_translator/attention_class_part_1.ipynb#ch0000035?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1081\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[39mif\u001b[39;00m output_gradients \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1079\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(output_gradients)]\n\u001b[0;32m-> 1081\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1082\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1083\u001b[0m     flat_targets,\n\u001b[1;32m   1084\u001b[0m     flat_sources,\n\u001b[1;32m   1085\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1086\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1087\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1089\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1090\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:156\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:1741\u001b[0m, in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1739\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m t_b:\n\u001b[1;32m   1740\u001b[0m   grad_a \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmat_mul(grad, b, transpose_b\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1741\u001b[0m   grad_b \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(a, grad, transpose_a\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m t_b:\n\u001b[1;32m   1743\u001b[0m   grad_a \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmat_mul(grad, b)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:6013\u001b[0m, in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6011\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   6012\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 6013\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   6014\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   6015\u001b[0m       transpose_b)\n\u001b[1;32m   6016\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6017\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "                                                        # inp: (batch_size, Tx)\n",
    "                                                        # targ: (batch_size, Ty)\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_sequential, enc_final = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_final\n",
    "            \n",
    "            dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)    # (m, 1)   \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_sequential) # predictions = (m, vocab_size)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # update dec_input for teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "    \n",
    "    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputs, encoder, decoder, max_source_length, max_target_length):\n",
    "    # inputs = (1, Tx)\n",
    "\n",
    "    \n",
    "    input_sentence = ''\n",
    "    for i in inputs[0]:\n",
    "        if i == 0:\n",
    "            break\n",
    "        input_sentence = input_sentence + source_sentence_tokenizer.index_word[i] + ' '\n",
    "    input_sentence = input_sentence[:-1]\n",
    "    \n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_sequential, enc_final = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_final\n",
    "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)             # dec_input = (1, 1)\n",
    "\n",
    "    # start decoding\n",
    "    for t in range(max_target_length): # limit the length of the decoded sequence\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_sequential)    # predictions = (1, vocab_size)\n",
    "                                                                                                       # dec_hidden = (1, units)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        # stop decoding if '<end>' is predicted\n",
    "        if target_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
    "            return result, input_sentence\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)                                         # dec_input = (1,1)  \n",
    "\n",
    "    return result, input_sentence\n",
    "  \n",
    "def predict_random_val_sentence():\n",
    "    \n",
    "    k = np.random.randint(len(source_train_tensor))\n",
    "    random_input = source_train_tensor[k]\n",
    "    random_output = target_train_tensor[k]\n",
    "    random_input = np.expand_dims(random_input,0)           # random_input = (1, Tx)\n",
    "    result, sentence = evaluate(random_input, encoder, decoder, max_source_length, max_target_length)\n",
    "    print(f'Input: {sentence[7:-5]}')                   # Want to skip \"start_ \" and \" _end\"\n",
    "    print(f'Predicted translation: {result[:-5]}')\n",
    "    true_translation = ''\n",
    "    for i in random_output:\n",
    "        if i == 0:\n",
    "            break\n",
    "        true_translation = true_translation + target_sentence_tokenizer.index_word[i] + ' '\n",
    "    true_translation = true_translation[7:-6]               # Want to skip \"start_\" and \" _end \"\n",
    "    print(f'Actual translation: {true_translation}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ shes very ill and has been in bed for a week _end\n",
      "Predicted translation: tom ist der einzige der einzige der einzige der einzige der einzige der einzige der einzige der einzige der einzige der einzige der einzige der \n",
      "Actual translation: start_ sie ist sehr krank und liegt schon seit einiger woche im bett _end \n",
      "None\n",
      "Input: start_ come on im taking you home _end\n",
      "Predicted translation: ich habe eine gute idee _end \n",
      "Actual translation: start_ komm ich bringe dich nach hause _end \n",
      "None\n",
      "Input: start_ in each beehive there can only be one queen _end\n",
      "Predicted translation: tom hat sich auf die ganze zeit _end \n",
      "Actual translation: start_ in jedem bienenstock kann es nur eine bienenkoenigin geben _end \n",
      "None\n",
      "Input: start_ im going to the bar _end\n",
      "Predicted translation: ich habe eine grosse spinne in der lage sein _end \n",
      "Actual translation: start_ ich gehe in die kneipe _end \n",
      "None\n",
      "Input: start_ its a wonder theyre still awake _end\n",
      "Predicted translation: tom ist ein guter fahrer _end \n",
      "Actual translation: start_ es ist ein wunder dass sie noch wach sind _end \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ now just relax _end\n",
      "Predicted translation: ich habe eine gute idee _end \n",
      "Actual translation: start_ jetzt entspann dich einfach _end \n",
      "None\n",
      "Input: start_ she asked him to give her some money so she could go to a restaurant with her friends _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ sie bat ihn um geld damit sie mit ihren freunden in ein restaurant gehen koennte _end \n",
      "None\n",
      "Input: start_ lets see what we remember from the last lesson _end\n",
      "Predicted translation: tom hat eine gute idee _end \n",
      "Actual translation: start_ wir wollen mal sehen was von der letzten unterrichtsstunde noch haengen geblieben ist _end \n",
      "None\n",
      "Input: start_ is this house yours _end\n",
      "Predicted translation: das ist _end \n",
      "Actual translation: start_ ist das dein haus _end \n",
      "None\n",
      "Input: start_ this car belongs to tom _end\n",
      "Predicted translation: tom ist nicht sehr _end \n",
      "Actual translation: start_ dieser wagen gehoert tom _end \n",
      "None\n",
      "Input: start_ i will give you a bike for your birthday _end\n",
      "Predicted translation: ich habe eine gute idee _end \n",
      "Actual translation: start_ ich werde dir ein fahrrad zum geburtstag schenken _end \n",
      "None\n",
      "Input: start_ she won an oscar nomination for best supporting actress _end\n",
      "Predicted translation: der tuer _end \n",
      "Actual translation: start_ sie wurde fuer einen oscar als beste nebendarstellerin nominiert _end \n",
      "None\n",
      "Input: start_ many of the students got bored and fell asleep _end\n",
      "Predicted translation: tom hat sich der nacht hellwach _end \n",
      "Actual translation: start_ viele schueler schliefen vor langeweile ein _end \n",
      "None\n",
      "Input: start_ i cant believe that youre really in love with me _end\n",
      "Predicted translation: ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich kann ich \n",
      "Actual translation: start_ ich kann nicht glauben dass du wirklich in mich verliebt bist _end \n",
      "None\n",
      "Input: start_ tom couldnt see mary _end\n",
      "Predicted translation: tom _end \n",
      "Actual translation: start_ tom konnte maria nicht sprechen _end \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ i thought you were older than me _end\n",
      "Predicted translation: ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich \n",
      "Actual translation: start_ ich dachte du waerest aelter als ich _end \n",
      "None\n",
      "Input: start_ these are real _end\n",
      "Predicted translation: das ist ein guter gitarrist _end \n",
      "Actual translation: start_ die sind echt _end \n",
      "None\n",
      "Input: start_ do you think i dont care _end\n",
      "Predicted translation: ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich \n",
      "Actual translation: start_ glaubst du etwa das ist mir gleich _end \n",
      "None\n",
      "Input: start_ i dont understand why people idolize criminals _end\n",
      "Predicted translation: ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich habe ich \n",
      "Actual translation: start_ ich verstehe nicht warum manche leute verbrecher verehren _end \n",
      "None\n",
      "Input: start_ the kitchen table was bare except for a bowl of fruit _end\n",
      "Predicted translation: der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der untergang der \n",
      "Actual translation: start_ abgesehen von einer schale obst war der kuechentisch leer _end \n",
      "None\n",
      "Input: start_ men arent so different from women _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ maenner unterscheiden sich gar nicht so sehr von frauen _end \n",
      "None\n",
      "Input: start_ one of these two answers is right _end\n",
      "Predicted translation: tom ist der tuer _end \n",
      "Actual translation: start_ eine dieser beiden antworten ist korrekt _end \n",
      "None\n",
      "Input: start_ weve known each other for a long time _end\n",
      "Predicted translation: tom ist der nacht bin _end \n",
      "Actual translation: start_ wir kennen uns schon sehr lange _end \n",
      "None\n",
      "Input: start_ maybe we could study together in the library _end\n",
      "Predicted translation: ich habe eine gute idee _end \n",
      "Actual translation: start_ vielleicht koennten wir in der bibliothek zusammen lernen _end \n",
      "None\n",
      "Input: start_ i shuffled the cards _end\n",
      "Predicted translation: ich trinke kaffee _end \n",
      "Actual translation: start_ ich mischte die karten _end \n",
      "None\n",
      "Input: start_ tom went shopping with his girlfriend _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ tom ging mit seiner freundin einkaufen _end \n",
      "None\n",
      "Input: start_ tom made a fool of himself in front of mary _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ tom hat sich vor maria laecherlich gemacht _end \n",
      "None\n",
      "Input: start_ im important _end\n",
      "Predicted translation: ich habe eine gute idee _end \n",
      "Actual translation: start_ ich bin wichtig _end \n",
      "None\n",
      "Input: start_ he did what his conscience dictated _end\n",
      "Predicted translation: tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom hat tom \n",
      "Actual translation: start_ er tat was sein gewissen ihm gebot _end \n",
      "None\n",
      "Input: start_ you arent rich are you _end\n",
      "Predicted translation: das ist sehr gut _end \n",
      "Actual translation: start_ sie sind nicht reich oder _end \n",
      "None\n",
      "Input: start_ tom wasnt really busy _end\n",
      "Predicted translation: tom _end \n",
      "Actual translation: start_ tom war nicht sehr beschaeftigt _end \n",
      "None\n",
      "Input: start_ i like it when tom does that _end\n",
      "Predicted translation: ich habe _end \n",
      "Actual translation: start_ ich mag es wenn tom das macht _end \n",
      "None\n",
      "Input: start_ if you trust such a fellow youll lose everything you have _end\n",
      "Predicted translation: was du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist \n",
      "Actual translation: start_ wenn sie sich so einem kerl anvertrauen verlieren sie alles was sie haben _end \n",
      "None\n",
      "Input: start_ tom teaches french to his friends _end\n",
      "Predicted translation: tom ist nicht sehr _end \n",
      "Actual translation: start_ tom unterrichtet seine freunde in franzoesisch _end \n",
      "None\n",
      "Input: start_ youve taken a long time eating lunch _end\n",
      "Predicted translation: was du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist du bist \n",
      "Actual translation: start_ du hast dir viel zeit beim mittagessen gelassen _end \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())\n",
    "print (predict_random_val_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
