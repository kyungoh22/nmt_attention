{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Notebook Overview </h2>\n",
    "\n",
    "- 1) Load and pre-process the data\n",
    "- 2) Pre-process data and split data into source / target arrays\n",
    "- 3) Create tokenizers for source and target languages using Keras' Tokenizer module\n",
    "- 4) Save tokenizers as json files\n",
    "- 5) Split data into training and test data: source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor\n",
    "- 6) Save training and test data as numpy arrays\n",
    "- 7) Create embedding matrices for source and target languages\n",
    "- 8) Save embedding matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import io\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Import from my own module\n",
    "from preprocessing import preprocess_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1) Load data from txt file </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])\n",
    "df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2) Pre-process sentences and divide data into source / target </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = df_en_de\n",
    "pairs = pairs.sample(frac = 1.00)\n",
    "pairs['german'] = pairs['german'].apply(preprocess_sentence)\n",
    "pairs['english'] = pairs['english'].apply(preprocess_sentence)\n",
    "\n",
    "source = pairs['german']\n",
    "target = pairs ['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251720"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3) Create word-based tokenizers + tokenize the sentences </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer for source language\n",
    "source_sentence_tokenizer = Tokenizer(filters='')\n",
    "# fit to source data\n",
    "source_sentence_tokenizer.fit_on_texts(source)\n",
    "# create tensor for source language -- every row is sequence of integers\n",
    "source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n",
    "# add zero padding to each sequence\n",
    "source_tensor = tf.keras.preprocessing.sequence.pad_sequences(source_tensor, padding='post' )\n",
    "\n",
    "# repeat for target language\n",
    "target_sentence_tokenizer = Tokenizer(filters='')\n",
    "target_sentence_tokenizer.fit_on_texts(target)\n",
    "target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4) Save tokenizers </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save tokenizers as json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentence_tokenizer_json = source_sentence_tokenizer.to_json()\n",
    "with io.open('tokenizers/source_sentence_tokenizer.json', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(json.dumps(source_sentence_tokenizer_json, ensure_ascii = False))\n",
    "\n",
    "target_sentence_tokenizer_json = target_sentence_tokenizer.to_json()\n",
    "with io.open('tokenizers/target_sentence_tokenizer.json', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(json.dumps(target_sentence_tokenizer_json, ensure_ascii = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create word-to-index and index-to-word mappings for source and target languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_index = source_sentence_tokenizer.word_index\n",
    "target_word_index = target_sentence_tokenizer.word_index\n",
    "\n",
    "source_index_word = source_sentence_tokenizer.index_word\n",
    "target_index_word = target_sentence_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5) Split data into train + test sets </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor = train_test_split(\n",
    "                                                                source_tensor, target_tensor,test_size=0.2\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 6) Save train and test sets (numpy arrays) as CSV files </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save numpy arrays as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as csv file:\n",
    "\n",
    "np.savetxt('tensors/source_train_tensor.csv', source_train_tensor, delimiter = ',')\n",
    "np.savetxt('tensors/source_test_tensor.csv', source_test_tensor, delimiter = ',')\n",
    "np.savetxt('tensors/target_train_tensor.csv', target_train_tensor, delimiter = ',')\n",
    "np.savetxt('tensors/target_test_tensor.csv', target_test_tensor, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 7) Create embedding matrices for source and target languages </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import German and English pipelines from spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jW7M1zstqJA",
    "outputId": "530bcdbf-5de0-4564-f9dc-8febae2e886f"
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download de_core_news_sm\n",
    "import de_core_news_sm\n",
    "\n",
    "#!python -m spacy download en_core_web_lg\n",
    "import en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "M0cck0b0tqJB"
   },
   "outputs": [],
   "source": [
    "nlp_source = de_core_news_sm.load()\n",
    "nlp_target = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define \"vocab_len_source\" and \"vocab_len_target\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8kqFuiStqJB",
    "outputId": "5a982029-1746-465b-fea7-9893f2eaddf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37347 17409\n"
     ]
    }
   ],
   "source": [
    "vocab_len_source = len(source_word_index.keys())\n",
    "vocab_len_target = len(target_word_index.keys())\n",
    "print (vocab_len_source, vocab_len_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add 1 for zero padding in embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3zeGasS4tqJC"
   },
   "outputs": [],
   "source": [
    "num_tokens_source = vocab_len_source + 1\n",
    "num_tokens_target = vocab_len_target + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vzBJZ91CtqJC"
   },
   "outputs": [],
   "source": [
    "# source language embedding dimensions\n",
    "embedding_dim_source = len(nlp_source('Der').vector)\n",
    "# initialise embedding matrix for source language\n",
    "# number of rows = number of tokens in source language\n",
    "embedding_matrix_source = np.zeros((num_tokens_source, embedding_dim_source))\n",
    "# for every word in source language\n",
    "for word, i in source_word_index.items():\n",
    "    # retrieve embedding vector\n",
    "    embedding_vector = nlp_source(word).vector\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "    if embedding_vector is not None:    \n",
    "        # insert embedding vector into row of embedding matrix\n",
    "        embedding_matrix_source[i] = embedding_vector\n",
    "\n",
    "# target language embedding dimensions\n",
    "embedding_dim_target = len(nlp_target('The').vector)\n",
    "# initialise embedding matrix for target language\n",
    "embedding_matrix_target = np.zeros((num_tokens_target, embedding_dim_target))\n",
    "for word, i in target_word_index.items():\n",
    "    embedding_vector = nlp_target(word).vector\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix_target[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run time for entire dataset: 2m 33s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 8) Save embedding matrices </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embedding matrices (numpy arrays) as csv files:\n",
    "np.savetxt('embeddings/embedding_matrix_source.csv', embedding_matrix_source, delimiter = ',')\n",
    "np.savetxt('embeddings/embedding_matrix_target.csv', embedding_matrix_target, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding matrices\n",
    "embedding_matrix_source = np.loadtxt('embeddings/embedding_matrix_source.csv', delimiter = ',', dtype = 'int32')\n",
    "embedding_matrix_target = np.loadtxt('embeddings/embedding_matrix_target.csv', delimiter = ',', dtype = 'int32')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a30d934768a106a0bdaa0b54b1b0ce58ac936216b32c4f047caaf50b54e34c32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
